{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW: X-ray images classification\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, open Mobaxterm and connect to triton with the user and password you were give with. Activate the environment `2ndPaper` and then type the command `pip install scikit-image`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will be dealing with classification of 32X32 X-ray images of the chest. The image can be classified into one of four options: lungs (l), clavicles (c), and heart (h) and background (b). Even though those labels are dependent, we will treat this task as multiclass and not as multilabel. The dataset for this assignment is located on a shared folder on triton (`/MLdata/MLcourse/X_ray/'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, MaxPool2D, Conv2D, Dropout\n",
    "from tensorflow.keras.layers import Flatten, InputLayer\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import *\n",
    "from skimage.io import imread\n",
    "\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "# %matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto(gpu_options =\n",
    "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "# device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(datapath):\n",
    "    # This part reads the images\n",
    "    classes = ['b','c','l','h']\n",
    "    imagelist = [fn for fn in os.listdir(datapath)]\n",
    "    N = len(imagelist)\n",
    "    num_classes = len(classes)\n",
    "    images = np.zeros((N, 32, 32, 1))\n",
    "    Y = np.zeros((N,num_classes))\n",
    "    ii=0\n",
    "    for fn in imagelist:\n",
    "\n",
    "        src = imread(os.path.join(datapath, fn),1)\n",
    "        img = resize(src,(32,32),order = 3)\n",
    "        \n",
    "        images[ii,:,:,0] = img\n",
    "        cc = -1\n",
    "        for cl in range(len(classes)):\n",
    "            if fn[-5] == classes[cl]:\n",
    "                cc = cl\n",
    "        Y[ii,cc]=1\n",
    "        ii += 1\n",
    "\n",
    "    BaseImages = images\n",
    "    BaseY = Y\n",
    "    return BaseImages, BaseY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_and_val(datapath):\n",
    "    # This part reads the images\n",
    "    classes = ['b','c','l','h']\n",
    "    imagelist = [fn for fn in os.listdir(datapath)]\n",
    "    N = len(imagelist)\n",
    "    num_classes = len(classes)\n",
    "    images = np.zeros((N, 32, 32, 1))\n",
    "    Y = np.zeros((N,num_classes))\n",
    "    ii=0\n",
    "    for fn in imagelist:\n",
    "\n",
    "        images[ii,:,:,0] = imread(os.path.join(datapath, fn),1)\n",
    "        cc = -1\n",
    "        for cl in range(len(classes)):\n",
    "            if fn[-5] == classes[cl]:\n",
    "                cc = cl\n",
    "        Y[ii,cc]=1\n",
    "        ii += 1\n",
    "\n",
    "    return images, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data for training and validation:\n",
    "src_data = 'X_ray/' #/MLdata/MLcourse/\n",
    "train_path = src_data + 'train'\n",
    "val_path = src_data + 'validation'\n",
    "test_path = src_data + 'test'\n",
    "BaseX_train , BaseY_train = preprocess_train_and_val(train_path)\n",
    "BaseX_val , BaseY_val = preprocess_train_and_val(val_path)\n",
    "X_test, Y_test = preprocess(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1: Fully connected layers \n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 1:***</span> *NN with fully connected layers. \n",
    "\n",
    "Elaborate a NN with 2 hidden fully connected layers with 300, 150 neurons and 4 neurons for classification. Use ReLU activation functions for the hidden layers and He_normal for initialization. Don't forget to flatten your image before feedforward to the first dense layer. Name the model `model_relu`.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "n_hidden_start=300\n",
    "model_relu = Sequential()\n",
    "model_relu.add(Flatten(input_shape=(32,32,)))\n",
    "model_relu.add(Dense(n_hidden_start,kernel_initializer=\"HeNormal\",activation='relu'))\n",
    "model_relu.add(Dense(int(n_hidden_start/2),kernel_initializer=\"HeNormal\", activation='relu'))\n",
    "model_relu.add(Dense(int(n_hidden_start/75),kernel_initializer=\"HeNormal\", activation='softmax'))\n",
    "\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               307500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 604       \n",
      "=================================================================\n",
      "Total params: 353,254\n",
      "Trainable params: 353,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_relu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: \n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "\n",
    "#Define your optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model with the optimizer above, accuracy metric and adequate loss for multiclass task. Train your model on the training set and evaluate the model on the testing set. Print the accuracy and loss over the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: results/init_weigths_relu\\assets\n",
      "Epoch 1/25\n",
      "102/102 [==============================] - 5s 36ms/step - loss: 1.3621 - accuracy: 0.3540 - val_loss: 1.1699 - val_accuracy: 0.6175\n",
      "Epoch 2/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.1346 - accuracy: 0.6496 - val_loss: 1.0513 - val_accuracy: 0.6858\n",
      "Epoch 3/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.0235 - accuracy: 0.7013 - val_loss: 0.9782 - val_accuracy: 0.7153\n",
      "Epoch 4/25\n",
      "102/102 [==============================] - 1s 5ms/step - loss: 0.9557 - accuracy: 0.7192 - val_loss: 0.9266 - val_accuracy: 0.7274\n",
      "Epoch 5/25\n",
      "102/102 [==============================] - 1s 5ms/step - loss: 0.9014 - accuracy: 0.7409 - val_loss: 0.8855 - val_accuracy: 0.7407\n",
      "Epoch 6/25\n",
      "102/102 [==============================] - 1s 6ms/step - loss: 0.8627 - accuracy: 0.7485 - val_loss: 0.8522 - val_accuracy: 0.7448\n",
      "Epoch 7/25\n",
      "102/102 [==============================] - 1s 6ms/step - loss: 0.8308 - accuracy: 0.7601 - val_loss: 0.8233 - val_accuracy: 0.7564\n",
      "Epoch 8/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.8101 - accuracy: 0.7583 - val_loss: 0.7989 - val_accuracy: 0.7633\n",
      "Epoch 9/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.7727 - accuracy: 0.7683 - val_loss: 0.7719 - val_accuracy: 0.7697\n",
      "Epoch 10/25\n",
      "102/102 [==============================] - 1s 5ms/step - loss: 0.7413 - accuracy: 0.7918 - val_loss: 0.7515 - val_accuracy: 0.7703\n",
      "Epoch 11/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.7239 - accuracy: 0.7853 - val_loss: 0.7306 - val_accuracy: 0.7784\n",
      "Epoch 12/25\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 0.7057 - accuracy: 0.7872 - val_loss: 0.7114 - val_accuracy: 0.7818\n",
      "Epoch 13/25\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 0.6822 - accuracy: 0.8030 - val_loss: 0.6923 - val_accuracy: 0.7946\n",
      "Epoch 14/25\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 0.6735 - accuracy: 0.7960 - val_loss: 0.6758 - val_accuracy: 0.7957\n",
      "Epoch 15/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.6415 - accuracy: 0.8115 - val_loss: 0.6614 - val_accuracy: 0.7992\n",
      "Epoch 16/25\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 0.6394 - accuracy: 0.8134 - val_loss: 0.6464 - val_accuracy: 0.7986\n",
      "Epoch 17/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.6237 - accuracy: 0.8167 - val_loss: 0.6322 - val_accuracy: 0.8096\n",
      "Epoch 18/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.6059 - accuracy: 0.8179 - val_loss: 0.6194 - val_accuracy: 0.8061\n",
      "Epoch 19/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.5922 - accuracy: 0.8221 - val_loss: 0.6068 - val_accuracy: 0.8148\n",
      "Epoch 20/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.5908 - accuracy: 0.8221 - val_loss: 0.5947 - val_accuracy: 0.8200\n",
      "Epoch 21/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.5707 - accuracy: 0.8322 - val_loss: 0.5835 - val_accuracy: 0.8229\n",
      "Epoch 22/25\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 0.5553 - accuracy: 0.8367 - val_loss: 0.5715 - val_accuracy: 0.8241\n",
      "Epoch 23/25\n",
      "102/102 [==============================] - 1s 5ms/step - loss: 0.5407 - accuracy: 0.8396 - val_loss: 0.5627 - val_accuracy: 0.8275\n",
      "Epoch 24/25\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 0.5213 - accuracy: 0.8458 - val_loss: 0.5529 - val_accuracy: 0.8287\n",
      "Epoch 25/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.5269 - accuracy: 0.8491 - val_loss: 0.5437 - val_accuracy: 0.8328\n",
      "Evaluate on test data\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.7966 - accuracy: 0.6743\n",
      "test loss, test acc: [0.7966227531433105, 0.6742857098579407]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "model_relu.compile(optimizer=AdamOpt,metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "\n",
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"init_weigths_relu\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model_relu.save(model_path)\n",
    "\n",
    "history_relu = model_relu.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs,validation_data=(BaseX_val,BaseY_val))\n",
    "\n",
    "y_pred_test = model_relu.predict(X_test)\n",
    "print(\"Evaluate on test data\")\n",
    "results = model_relu.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 2:***</span> *Activation functions.* \n",
    "\n",
    "Change the activation functions to LeakyRelu or tanh or sigmoid. Name the new model `new_a_model`. Explain how it can affect the model.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "n_hidden_start=300\n",
    "new_a_model = Sequential()\n",
    "new_a_model.add(Flatten(input_shape=(32,32,)))\n",
    "new_a_model.add(Dense(n_hidden_start,kernel_initializer=\"HeNormal\",activation='sigmoid'))\n",
    "new_a_model.add(Dense(int(n_hidden_start/2),kernel_initializer=\"HeNormal\", activation='sigmoid'))\n",
    "new_a_model.add(Dense(int(n_hidden_start/75),kernel_initializer=\"HeNormal\", activation='softmax'))\n",
    "\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               307500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 604       \n",
      "=================================================================\n",
      "Total params: 353,254\n",
      "Trainable params: 353,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_a_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 3:***</span> *Number of epochs.* \n",
    "\n",
    "Train the new model using 25 and 40 epochs. What difference does it makes in term of performance? Remember to save the compiled model for having initialized weights for every run as we did in tutorial 12. Evaluate each trained model on the test set*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: \n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: results/init_weigths_newmodel\\assets\n",
      "Epoch 1/25\n",
      "102/102 [==============================] - 1s 8ms/step - loss: 1.7324 - accuracy: 0.2537 - val_loss: 1.5383 - val_accuracy: 0.2500\n",
      "Epoch 2/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.4940 - accuracy: 0.2536 - val_loss: 1.3860 - val_accuracy: 0.3119\n",
      "Epoch 3/25\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 1.3754 - accuracy: 0.3327 - val_loss: 1.3326 - val_accuracy: 0.4167\n",
      "Epoch 4/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.3312 - accuracy: 0.4343 - val_loss: 1.3103 - val_accuracy: 0.5579\n",
      "Epoch 5/25\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 1.3109 - accuracy: 0.5409 - val_loss: 1.2929 - val_accuracy: 0.5538\n",
      "Epoch 6/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.2950 - accuracy: 0.5499 - val_loss: 1.2762 - val_accuracy: 0.5573\n",
      "Epoch 7/25\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 1.2785 - accuracy: 0.5376 - val_loss: 1.2593 - val_accuracy: 0.5747\n",
      "Epoch 8/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.2612 - accuracy: 0.5568 - val_loss: 1.2428 - val_accuracy: 0.5712\n",
      "Epoch 9/25\n",
      "102/102 [==============================] - 1s 5ms/step - loss: 1.2390 - accuracy: 0.5582 - val_loss: 1.2264 - val_accuracy: 0.5810\n",
      "Epoch 10/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.2298 - accuracy: 0.5528 - val_loss: 1.2103 - val_accuracy: 0.5862\n",
      "Epoch 11/25\n",
      "102/102 [==============================] - ETA: 0s - loss: 1.2160 - accuracy: 0.57 - 0s 5ms/step - loss: 1.2157 - accuracy: 0.5745 - val_loss: 1.1947 - val_accuracy: 0.5741\n",
      "Epoch 12/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.1937 - accuracy: 0.5696 - val_loss: 1.1792 - val_accuracy: 0.5816\n",
      "Epoch 13/25\n",
      "102/102 [==============================] - 1s 5ms/step - loss: 1.1839 - accuracy: 0.5614 - val_loss: 1.1644 - val_accuracy: 0.5920\n",
      "Epoch 14/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.1729 - accuracy: 0.5726 - val_loss: 1.1501 - val_accuracy: 0.5856\n",
      "Epoch 15/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.1515 - accuracy: 0.5713 - val_loss: 1.1365 - val_accuracy: 0.5897\n",
      "Epoch 16/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.1341 - accuracy: 0.5756 - val_loss: 1.1238 - val_accuracy: 0.6042\n",
      "Epoch 17/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.1315 - accuracy: 0.5949 - val_loss: 1.1114 - val_accuracy: 0.6053\n",
      "Epoch 18/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.1127 - accuracy: 0.5885 - val_loss: 1.0996 - val_accuracy: 0.6047\n",
      "Epoch 19/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.0967 - accuracy: 0.5890 - val_loss: 1.0887 - val_accuracy: 0.6140\n",
      "Epoch 20/25\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 1.0913 - accuracy: 0.5955 - val_loss: 1.0779 - val_accuracy: 0.6134\n",
      "Epoch 21/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.0809 - accuracy: 0.5939 - val_loss: 1.0679 - val_accuracy: 0.6157\n",
      "Epoch 22/25\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.0726 - accuracy: 0.6017 - val_loss: 1.0586 - val_accuracy: 0.6146\n",
      "Epoch 23/25\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 1.0500 - accuracy: 0.6134 - val_loss: 1.0499 - val_accuracy: 0.6186\n",
      "Epoch 24/25\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 1.0554 - accuracy: 0.6027 - val_loss: 1.0414 - val_accuracy: 0.6273\n",
      "Epoch 25/25\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 1.0452 - accuracy: 0.6083 - val_loss: 1.0330 - val_accuracy: 0.6383\n",
      "Evaluate on test data\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.1275 - accuracy: 0.5371\n",
      "test loss, test acc: [1.1274737119674683, 0.5371428728103638]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "new_a_model.compile(optimizer=AdamOpt,metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "# Code taken from TUTORIAL 11:::::\n",
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"init_weigths_newmodel\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "new_a_model.save(model_path)\n",
    "# print('Saved initialized model at %s ' % model_path)\n",
    "\n",
    "history_model_a = new_a_model.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs,validation_data = (BaseX_val,BaseY_val))\n",
    "\n",
    "y_pred_test = new_a_model.predict(X_test)\n",
    "print(\"Evaluate on test data\")\n",
    "results = new_a_model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: \n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "batch_size = 64\n",
    "epochs = 40\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "102/102 [==============================] - 2s 8ms/step - loss: 1.6716 - accuracy: 0.2502 - val_loss: 1.5367 - val_accuracy: 0.2500\n",
      "Epoch 2/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.4564 - accuracy: 0.2610 - val_loss: 1.3865 - val_accuracy: 0.2703\n",
      "Epoch 3/40\n",
      "102/102 [==============================] - 1s 6ms/step - loss: 1.3597 - accuracy: 0.3350 - val_loss: 1.3324 - val_accuracy: 0.4149\n",
      "Epoch 4/40\n",
      "102/102 [==============================] - 1s 6ms/step - loss: 1.3262 - accuracy: 0.4872 - val_loss: 1.3107 - val_accuracy: 0.5590\n",
      "Epoch 5/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.3073 - accuracy: 0.5432 - val_loss: 1.2935 - val_accuracy: 0.5602\n",
      "Epoch 6/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.2905 - accuracy: 0.5573 - val_loss: 1.2767 - val_accuracy: 0.5671\n",
      "Epoch 7/40\n",
      "102/102 [==============================] - 1s 6ms/step - loss: 1.2738 - accuracy: 0.5564 - val_loss: 1.2598 - val_accuracy: 0.5712\n",
      "Epoch 8/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.2568 - accuracy: 0.5511 - val_loss: 1.2428 - val_accuracy: 0.5816\n",
      "Epoch 9/40\n",
      "102/102 [==============================] - 1s 7ms/step - loss: 1.2403 - accuracy: 0.5596 - val_loss: 1.2263 - val_accuracy: 0.5822\n",
      "Epoch 10/40\n",
      "102/102 [==============================] - 1s 8ms/step - loss: 1.2240 - accuracy: 0.5598 - val_loss: 1.2100 - val_accuracy: 0.5891\n",
      "Epoch 11/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.2080 - accuracy: 0.5704 - val_loss: 1.1942 - val_accuracy: 0.5718\n",
      "Epoch 12/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.1922 - accuracy: 0.5638 - val_loss: 1.1785 - val_accuracy: 0.5903\n",
      "Epoch 13/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.1773 - accuracy: 0.5760 - val_loss: 1.1635 - val_accuracy: 0.5920\n",
      "Epoch 14/40\n",
      "102/102 [==============================] - 1s 7ms/step - loss: 1.1629 - accuracy: 0.5820 - val_loss: 1.1493 - val_accuracy: 0.5949\n",
      "Epoch 15/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.1488 - accuracy: 0.5819 - val_loss: 1.1355 - val_accuracy: 0.5961\n",
      "Epoch 16/40\n",
      "102/102 [==============================] - 1s 5ms/step - loss: 1.1353 - accuracy: 0.5816 - val_loss: 1.1226 - val_accuracy: 0.6001\n",
      "Epoch 17/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.1226 - accuracy: 0.5848 - val_loss: 1.1104 - val_accuracy: 0.6047\n",
      "Epoch 18/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.1105 - accuracy: 0.5877 - val_loss: 1.0983 - val_accuracy: 0.6105\n",
      "Epoch 19/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.0992 - accuracy: 0.5899 - val_loss: 1.0873 - val_accuracy: 0.6094\n",
      "Epoch 20/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.0883 - accuracy: 0.5935 - val_loss: 1.0766 - val_accuracy: 0.6128\n",
      "Epoch 21/40\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 1.0780 - accuracy: 0.5945 - val_loss: 1.0667 - val_accuracy: 0.6152\n",
      "Epoch 22/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.0683 - accuracy: 0.6013 - val_loss: 1.0573 - val_accuracy: 0.6181\n",
      "Epoch 23/40\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 1.0589 - accuracy: 0.5992 - val_loss: 1.0487 - val_accuracy: 0.6279\n",
      "Epoch 24/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.0500 - accuracy: 0.6114 - val_loss: 1.0401 - val_accuracy: 0.6209\n",
      "Epoch 25/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.0417 - accuracy: 0.6104 - val_loss: 1.0319 - val_accuracy: 0.6256\n",
      "Epoch 26/40\n",
      "102/102 [==============================] - ETA: 0s - loss: 1.0332 - accuracy: 0.61 - 1s 5ms/step - loss: 1.0338 - accuracy: 0.6165 - val_loss: 1.0244 - val_accuracy: 0.6348\n",
      "Epoch 27/40\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 1.0260 - accuracy: 0.6202 - val_loss: 1.0171 - val_accuracy: 0.6238\n",
      "Epoch 28/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 1.0188 - accuracy: 0.6200 - val_loss: 1.0101 - val_accuracy: 0.6314\n",
      "Epoch 29/40\n",
      "102/102 [==============================] - 1s 9ms/step - loss: 1.0118 - accuracy: 0.6279 - val_loss: 1.0036 - val_accuracy: 0.6331\n",
      "Epoch 30/40\n",
      "102/102 [==============================] - 1s 6ms/step - loss: 1.0050 - accuracy: 0.6270 - val_loss: 0.9970 - val_accuracy: 0.6435\n",
      "Epoch 31/40\n",
      "102/102 [==============================] - 1s 7ms/step - loss: 0.9982 - accuracy: 0.6259 - val_loss: 0.9910 - val_accuracy: 0.6464\n",
      "Epoch 32/40\n",
      "102/102 [==============================] - 1s 10ms/step - loss: 0.9922 - accuracy: 0.6362 - val_loss: 0.9852 - val_accuracy: 0.6487\n",
      "Epoch 33/40\n",
      "102/102 [==============================] - 1s 6ms/step - loss: 0.9858 - accuracy: 0.6301 - val_loss: 0.9798 - val_accuracy: 0.6528\n",
      "Epoch 34/40\n",
      "102/102 [==============================] - 1s 6ms/step - loss: 0.9800 - accuracy: 0.6372 - val_loss: 0.9737 - val_accuracy: 0.6568\n",
      "Epoch 35/40\n",
      "102/102 [==============================] - ETA: 0s - loss: 0.9748 - accuracy: 0.63 - 0s 5ms/step - loss: 0.9741 - accuracy: 0.6367 - val_loss: 0.9682 - val_accuracy: 0.6551\n",
      "Epoch 36/40\n",
      "102/102 [==============================] - 1s 6ms/step - loss: 0.9684 - accuracy: 0.6432 - val_loss: 0.9629 - val_accuracy: 0.6539\n",
      "Epoch 37/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.9632 - accuracy: 0.6424 - val_loss: 0.9582 - val_accuracy: 0.6649\n",
      "Epoch 38/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.9580 - accuracy: 0.6474 - val_loss: 0.9529 - val_accuracy: 0.6649\n",
      "Epoch 39/40\n",
      "102/102 [==============================] - 0s 4ms/step - loss: 0.9525 - accuracy: 0.6503 - val_loss: 0.9481 - val_accuracy: 0.6649\n",
      "Epoch 40/40\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 0.9475 - accuracy: 0.6540 - val_loss: 0.9431 - val_accuracy: 0.6597\n",
      "Evaluate on test data\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.0168 - accuracy: 0.5886\n",
      "test loss, test acc: [1.0168415307998657, 0.5885714292526245]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "new_a_model.compile(optimizer=AdamOpt,metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "\n",
    "new_a_model = tf.keras.models.load_model(\"results/init_weigths_newmodel\") # Initializing weights before total run\n",
    "\n",
    "history_model_a = new_a_model.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs,validation_data=(BaseX_val,BaseY_val))\n",
    "\n",
    "y_pred_test = new_a_model.predict(X_test)\n",
    "print(\"Evaluate on test data\")\n",
    "results = new_a_model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "print(\"test loss, test acc:\", results)\n",
    "#-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 4:***</span> *Mini-batches.* \n",
    "\n",
    "Build the `model_relu` again and run it with a batch size of 32 instead of 64. What are the advantages of the mini-batch vs. SGD?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "n_hidden_start=300\n",
    "model_relu = Sequential()\n",
    "model_relu.add(Flatten(input_shape=(32,32,)))\n",
    "model_relu.add(Dense(n_hidden_start,kernel_initializer=\"HeNormal\", activation='relu'))\n",
    "model_relu.add(Dense(int(n_hidden_start/2),kernel_initializer=\"HeNormal\", activation='relu'))\n",
    "model_relu.add(Dense(int(n_hidden_start/75),kernel_initializer=\"HeNormal\", activation='softmax'))\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "#Define your optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "203/203 [==============================] - 2s 5ms/step - loss: 1.2088 - accuracy: 0.5392 - val_loss: 1.0809 - val_accuracy: 0.6829\n",
      "Epoch 2/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 1.0089 - accuracy: 0.7051 - val_loss: 0.9643 - val_accuracy: 0.6979\n",
      "Epoch 3/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.9174 - accuracy: 0.7314 - val_loss: 0.8922 - val_accuracy: 0.7396\n",
      "Epoch 4/50\n",
      "203/203 [==============================] - 1s 6ms/step - loss: 0.8543 - accuracy: 0.7488 - val_loss: 0.8418 - val_accuracy: 0.7523\n",
      "Epoch 5/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.8063 - accuracy: 0.7617 - val_loss: 0.7987 - val_accuracy: 0.7627\n",
      "Epoch 6/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.7635 - accuracy: 0.7766 - val_loss: 0.7638 - val_accuracy: 0.7720\n",
      "Epoch 7/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.7277 - accuracy: 0.7879 - val_loss: 0.7303 - val_accuracy: 0.7830\n",
      "Epoch 8/50\n",
      "203/203 [==============================] - 1s 7ms/step - loss: 0.6974 - accuracy: 0.7933 - val_loss: 0.7012 - val_accuracy: 0.7905\n",
      "Epoch 9/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.6688 - accuracy: 0.8012 - val_loss: 0.6771 - val_accuracy: 0.7917\n",
      "Epoch 10/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.6450 - accuracy: 0.8055 - val_loss: 0.6518 - val_accuracy: 0.8050\n",
      "Epoch 11/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.6221 - accuracy: 0.8151 - val_loss: 0.6322 - val_accuracy: 0.8084\n",
      "Epoch 12/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.6005 - accuracy: 0.8188 - val_loss: 0.6111 - val_accuracy: 0.8125\n",
      "Epoch 13/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5816 - accuracy: 0.8251 - val_loss: 0.5939 - val_accuracy: 0.8212\n",
      "Epoch 14/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5643 - accuracy: 0.8290 - val_loss: 0.5786 - val_accuracy: 0.8235\n",
      "Epoch 15/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.5475 - accuracy: 0.8336 - val_loss: 0.5645 - val_accuracy: 0.8247\n",
      "Epoch 16/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.5326 - accuracy: 0.8404 - val_loss: 0.5496 - val_accuracy: 0.8293\n",
      "Epoch 17/50\n",
      "203/203 [==============================] - 1s 6ms/step - loss: 0.5185 - accuracy: 0.8448 - val_loss: 0.5390 - val_accuracy: 0.8333\n",
      "Epoch 18/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.5069 - accuracy: 0.8488 - val_loss: 0.5270 - val_accuracy: 0.8356\n",
      "Epoch 19/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.4936 - accuracy: 0.8530 - val_loss: 0.5156 - val_accuracy: 0.8380\n",
      "Epoch 20/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.4827 - accuracy: 0.8537 - val_loss: 0.5012 - val_accuracy: 0.8426\n",
      "Epoch 21/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4723 - accuracy: 0.8590 - val_loss: 0.4920 - val_accuracy: 0.8443\n",
      "Epoch 22/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.4621 - accuracy: 0.8599 - val_loss: 0.4860 - val_accuracy: 0.8449\n",
      "Epoch 23/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4531 - accuracy: 0.8645 - val_loss: 0.4746 - val_accuracy: 0.8507\n",
      "Epoch 24/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4439 - accuracy: 0.8664 - val_loss: 0.4682 - val_accuracy: 0.8478\n",
      "Epoch 25/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4349 - accuracy: 0.8675 - val_loss: 0.4577 - val_accuracy: 0.8530\n",
      "Epoch 26/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4264 - accuracy: 0.8712 - val_loss: 0.4505 - val_accuracy: 0.8553\n",
      "Epoch 27/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.4185 - accuracy: 0.8730 - val_loss: 0.4408 - val_accuracy: 0.8605\n",
      "Epoch 28/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.4117 - accuracy: 0.8741 - val_loss: 0.4363 - val_accuracy: 0.8605\n",
      "Epoch 29/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.4042 - accuracy: 0.8761 - val_loss: 0.4303 - val_accuracy: 0.8646\n",
      "Epoch 30/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.3983 - accuracy: 0.8772 - val_loss: 0.4254 - val_accuracy: 0.8657\n",
      "Epoch 31/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3923 - accuracy: 0.8803 - val_loss: 0.4190 - val_accuracy: 0.8652\n",
      "Epoch 32/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3860 - accuracy: 0.8818 - val_loss: 0.4119 - val_accuracy: 0.8704\n",
      "Epoch 33/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3804 - accuracy: 0.8808 - val_loss: 0.4074 - val_accuracy: 0.8692\n",
      "Epoch 34/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3746 - accuracy: 0.8842 - val_loss: 0.4029 - val_accuracy: 0.8727\n",
      "Epoch 35/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3691 - accuracy: 0.8863 - val_loss: 0.3971 - val_accuracy: 0.8738\n",
      "Epoch 36/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3638 - accuracy: 0.8879 - val_loss: 0.3928 - val_accuracy: 0.8733\n",
      "Epoch 37/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3592 - accuracy: 0.8876 - val_loss: 0.3896 - val_accuracy: 0.8762\n",
      "Epoch 38/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3551 - accuracy: 0.8883 - val_loss: 0.3872 - val_accuracy: 0.8767\n",
      "Epoch 39/50\n",
      "203/203 [==============================] - 1s 6ms/step - loss: 0.3509 - accuracy: 0.8897 - val_loss: 0.3792 - val_accuracy: 0.8785\n",
      "Epoch 40/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3455 - accuracy: 0.8905 - val_loss: 0.3751 - val_accuracy: 0.8831\n",
      "Epoch 41/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3421 - accuracy: 0.8939 - val_loss: 0.3713 - val_accuracy: 0.8819\n",
      "Epoch 42/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3379 - accuracy: 0.8908 - val_loss: 0.3690 - val_accuracy: 0.8796\n",
      "Epoch 43/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3333 - accuracy: 0.8962 - val_loss: 0.3646 - val_accuracy: 0.8837\n",
      "Epoch 44/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3298 - accuracy: 0.8936 - val_loss: 0.3611 - val_accuracy: 0.8860\n",
      "Epoch 45/50\n",
      "203/203 [==============================] - 1s 3ms/step - loss: 0.3256 - accuracy: 0.8973 - val_loss: 0.3551 - val_accuracy: 0.8929\n",
      "Epoch 46/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.3228 - accuracy: 0.8979 - val_loss: 0.3539 - val_accuracy: 0.8889\n",
      "Epoch 47/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3190 - accuracy: 0.8993 - val_loss: 0.3525 - val_accuracy: 0.8895\n",
      "Epoch 48/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.3149 - accuracy: 0.8999 - val_loss: 0.3498 - val_accuracy: 0.8889\n",
      "Epoch 49/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3122 - accuracy: 0.9033 - val_loss: 0.3479 - val_accuracy: 0.8895\n",
      "Epoch 50/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.3092 - accuracy: 0.9022 - val_loss: 0.3438 - val_accuracy: 0.8906\n",
      "Evaluate on test data\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8311 - accuracy: 0.6686\n",
      "test loss, test acc: [0.8311223387718201, 0.668571412563324]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "model_relu.compile(optimizer=AdamOpt,metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "\n",
    "model_relu = tf.keras.models.load_model(\"results/init_weigths_relu\")\n",
    "history_relu = model_relu.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs,validation_data=(BaseX_val,BaseY_val))\n",
    "\n",
    "y_pred_test = model_relu.predict(X_test)\n",
    "print(\"Evaluate on test data\")\n",
    "results = model_relu.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "print(\"test loss, test acc:\", results)\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 4:***</span> *Batch normalization.* \n",
    "\n",
    "Build the `new_a_model` again and add batch normalization layers. How does it impact your results?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "n_hidden_start=300\n",
    "new_a_model = Sequential()\n",
    "new_a_model.add(Flatten(input_shape=(32,32,)))\n",
    "new_a_model.add(Dense(n_hidden_start,kernel_initializer=\"HeNormal\",activation='sigmoid'))\n",
    "new_a_model.add(BatchNormalization())\n",
    "new_a_model.add(Dense(int(n_hidden_start/2), kernel_initializer=\"HeNormal\",activation='sigmoid'))\n",
    "new_a_model.add(BatchNormalization())\n",
    "new_a_model.add(Dense(int(n_hidden_start/75), kernel_initializer=\"HeNormal\",activation='softmax'))\n",
    "new_a_model.add(BatchNormalization())\n",
    "#---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "#Define your optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "#Compile the network: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "203/203 [==============================] - 2s 5ms/step - loss: 1.5685 - accuracy: 0.2607 - val_loss: 1.3961 - val_accuracy: 0.3345\n",
      "Epoch 2/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 1.3497 - accuracy: 0.4359 - val_loss: 1.3170 - val_accuracy: 0.4531\n",
      "Epoch 3/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 1.3074 - accuracy: 0.5222 - val_loss: 1.2876 - val_accuracy: 0.5440\n",
      "Epoch 4/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 1.2788 - accuracy: 0.5494 - val_loss: 1.2592 - val_accuracy: 0.5804\n",
      "Epoch 5/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 1.2513 - accuracy: 0.5624 - val_loss: 1.2318 - val_accuracy: 0.5671\n",
      "Epoch 6/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 1.2245 - accuracy: 0.5573 - val_loss: 1.2052 - val_accuracy: 0.5833\n",
      "Epoch 7/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 1.1987 - accuracy: 0.5652 - val_loss: 1.1799 - val_accuracy: 0.5868\n",
      "Epoch 8/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 1.1742 - accuracy: 0.5692 - val_loss: 1.1565 - val_accuracy: 0.6024\n",
      "Epoch 9/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 1.1513 - accuracy: 0.5803 - val_loss: 1.1339 - val_accuracy: 0.6001\n",
      "Epoch 10/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 1.1302 - accuracy: 0.5842 - val_loss: 1.1138 - val_accuracy: 0.6053\n",
      "Epoch 11/50\n",
      "203/203 [==============================] - 1s 6ms/step - loss: 1.1110 - accuracy: 0.5870 - val_loss: 1.0950 - val_accuracy: 0.6117\n",
      "Epoch 12/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 1.0929 - accuracy: 0.5930 - val_loss: 1.0779 - val_accuracy: 0.6181\n",
      "Epoch 13/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 1.0766 - accuracy: 0.5990 - val_loss: 1.0626 - val_accuracy: 0.6204\n",
      "Epoch 14/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 1.0612 - accuracy: 0.6052 - val_loss: 1.0489 - val_accuracy: 0.6267\n",
      "Epoch 15/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 1.0475 - accuracy: 0.6057 - val_loss: 1.0353 - val_accuracy: 0.6366\n",
      "Epoch 16/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 1.0348 - accuracy: 0.6169 - val_loss: 1.0233 - val_accuracy: 0.6285\n",
      "Epoch 17/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 1.0229 - accuracy: 0.6171 - val_loss: 1.0119 - val_accuracy: 0.6348\n",
      "Epoch 18/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 1.0116 - accuracy: 0.6247 - val_loss: 1.0019 - val_accuracy: 0.6366\n",
      "Epoch 19/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 1.0012 - accuracy: 0.6273 - val_loss: 0.9916 - val_accuracy: 0.6406\n",
      "Epoch 20/50\n",
      "203/203 [==============================] - 1s 6ms/step - loss: 0.9912 - accuracy: 0.6310 - val_loss: 0.9824 - val_accuracy: 0.6435\n",
      "Epoch 21/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.9818 - accuracy: 0.6387 - val_loss: 0.9735 - val_accuracy: 0.6458\n",
      "Epoch 22/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.9726 - accuracy: 0.6384 - val_loss: 0.9657 - val_accuracy: 0.6574\n",
      "Epoch 23/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.9645 - accuracy: 0.6430 - val_loss: 0.9572 - val_accuracy: 0.6580\n",
      "Epoch 24/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.9562 - accuracy: 0.6494 - val_loss: 0.9496 - val_accuracy: 0.6591\n",
      "Epoch 25/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.9482 - accuracy: 0.6523 - val_loss: 0.9425 - val_accuracy: 0.6609\n",
      "Epoch 26/50\n",
      "203/203 [==============================] - 1s 6ms/step - loss: 0.9407 - accuracy: 0.6568 - val_loss: 0.9356 - val_accuracy: 0.6672\n",
      "Epoch 27/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.9331 - accuracy: 0.6574 - val_loss: 0.9285 - val_accuracy: 0.6730\n",
      "Epoch 28/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.9263 - accuracy: 0.6654 - val_loss: 0.9222 - val_accuracy: 0.6742\n",
      "Epoch 29/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.9194 - accuracy: 0.6639 - val_loss: 0.9155 - val_accuracy: 0.6771\n",
      "Epoch 30/50\n",
      "203/203 [==============================] - 1s 6ms/step - loss: 0.9123 - accuracy: 0.6739 - val_loss: 0.9096 - val_accuracy: 0.6696\n",
      "Epoch 31/50\n",
      "203/203 [==============================] - 1s 6ms/step - loss: 0.9056 - accuracy: 0.6766 - val_loss: 0.9031 - val_accuracy: 0.6759\n",
      "Epoch 32/50\n",
      "203/203 [==============================] - 1s 3ms/step - loss: 0.8992 - accuracy: 0.6789 - val_loss: 0.8982 - val_accuracy: 0.6794\n",
      "Epoch 33/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8929 - accuracy: 0.6801 - val_loss: 0.8918 - val_accuracy: 0.6834\n",
      "Epoch 34/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8867 - accuracy: 0.6860 - val_loss: 0.8858 - val_accuracy: 0.6869\n",
      "Epoch 35/50\n",
      "203/203 [==============================] - 1s 6ms/step - loss: 0.8806 - accuracy: 0.6867 - val_loss: 0.8805 - val_accuracy: 0.6881\n",
      "Epoch 36/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8749 - accuracy: 0.6886 - val_loss: 0.8749 - val_accuracy: 0.6898\n",
      "Epoch 37/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8688 - accuracy: 0.6912 - val_loss: 0.8701 - val_accuracy: 0.6956\n",
      "Epoch 38/50\n",
      "203/203 [==============================] - 1s 6ms/step - loss: 0.8633 - accuracy: 0.6918 - val_loss: 0.8647 - val_accuracy: 0.6997\n",
      "Epoch 39/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8574 - accuracy: 0.6948 - val_loss: 0.8601 - val_accuracy: 0.6916\n",
      "Epoch 40/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8523 - accuracy: 0.6951 - val_loss: 0.8544 - val_accuracy: 0.6979\n",
      "Epoch 41/50\n",
      "203/203 [==============================] - 1s 6ms/step - loss: 0.8467 - accuracy: 0.6982 - val_loss: 0.8501 - val_accuracy: 0.6950\n",
      "Epoch 42/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.8412 - accuracy: 0.6993 - val_loss: 0.8456 - val_accuracy: 0.7002\n",
      "Epoch 43/50\n",
      "203/203 [==============================] - 1s 3ms/step - loss: 0.8361 - accuracy: 0.7005 - val_loss: 0.8404 - val_accuracy: 0.7043\n",
      "Epoch 44/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8312 - accuracy: 0.7027 - val_loss: 0.8355 - val_accuracy: 0.7095\n",
      "Epoch 45/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8261 - accuracy: 0.7047 - val_loss: 0.8318 - val_accuracy: 0.7089\n",
      "Epoch 46/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8211 - accuracy: 0.7076 - val_loss: 0.8264 - val_accuracy: 0.7112\n",
      "Epoch 47/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8162 - accuracy: 0.7088 - val_loss: 0.8226 - val_accuracy: 0.7124\n",
      "Epoch 48/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8116 - accuracy: 0.7091 - val_loss: 0.8178 - val_accuracy: 0.7124\n",
      "Epoch 49/50\n",
      "203/203 [==============================] - 1s 5ms/step - loss: 0.8070 - accuracy: 0.7127 - val_loss: 0.8139 - val_accuracy: 0.7101\n",
      "Epoch 50/50\n",
      "203/203 [==============================] - 1s 4ms/step - loss: 0.8022 - accuracy: 0.7133 - val_loss: 0.8097 - val_accuracy: 0.7124\n",
      "Evaluate on test data\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8946 - accuracy: 0.6343\n",
      "test loss, test acc: [0.8946154713630676, 0.6342856884002686]\n"
     ]
    }
   ],
   "source": [
    "#Preforming the training by using fit \n",
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "new_a_model.compile(optimizer=AdamOpt,metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "new_a_model = tf.keras.models.load_model(\"results/init_weigths_newmodel\") \n",
    "history_model_a = new_a_model.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs,validation_data = (BaseX_val,BaseY_val))\n",
    "\n",
    "y_pred_test = new_a_model.predict(X_test)\n",
    "print(\"Evaluate on test data\")\n",
    "results = new_a_model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "print(\"test loss, test acc:\", results)\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2: Convolutional Neural Network (CNN)\n",
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 1:***</span> *2D CNN.* \n",
    "\n",
    "Have a look at the model below and answer the following:\n",
    "* How many layers does it have?\n",
    "* How many filter in each layer?\n",
    "* Would the number of parmaters be similar to a fully connected NN?\n",
    "* Is this specific NN performing regularization?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(input_shape,drop,dropRate,reg):\n",
    "    #Defining the network architecture:\n",
    "    model = Sequential()\n",
    "    model.add(Permute((1,2,3),input_shape = input_shape))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_1',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_2',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:    \n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_3',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_4',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_5',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    #Fully connected network tail:      \n",
    "    model.add(Dense(512, activation='elu',name='FCN_1')) \n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(Dense(128, activation='elu',name='FCN_2'))\n",
    "    model.add(Dense(4, activation= 'softmax',name='FCN_3'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1 (Conv2D)            (None, 32, 32, 64)        640       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "Conv2D_5 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "FCN_1 (Dense)                (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "FCN_2 (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "FCN_3 (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 3,271,492\n",
      "Trainable params: 3,271,332\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 1e-03\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "drop = True\n",
    "dropRate = 0.3\n",
    "reg = 1e-2\n",
    "NNet = get_net(input_shape,drop,dropRate,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_1 (Permute)          (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1 (Conv2D)            (None, 32, 32, 64)        640       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 64)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "Conv2D_5 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "FCN_1 (Dense)                (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "FCN_2 (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "FCN_3 (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 3,271,492\n",
      "Trainable params: 3,271,332\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NNet=get_net(input_shape,drop,dropRate,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import *\n",
    "import os\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "\n",
    "#Compile the network: \n",
    "NNet.compile(optimizer=AdamOpt, metrics=['acc'], loss='categorical_crossentropy')\n",
    "\n",
    "#Saving checkpoints during training:\n",
    "Checkpath = os.getcwd()\n",
    "Checkp = ModelCheckpoint(Checkpath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, save_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "102/102 [==============================] - 51s 492ms/step - loss: 8.2400 - acc: 0.3780 - val_loss: 8.0294 - val_acc: 0.2500\n",
      "Epoch 2/25\n",
      "102/102 [==============================] - 49s 476ms/step - loss: 7.6110 - acc: 0.5273 - val_loss: 8.1661 - val_acc: 0.2500\n",
      "Epoch 3/25\n",
      "102/102 [==============================] - 49s 484ms/step - loss: 7.4201 - acc: 0.5844 - val_loss: 8.2606 - val_acc: 0.2575\n",
      "Epoch 4/25\n",
      "102/102 [==============================] - 49s 479ms/step - loss: 7.3105 - acc: 0.6202 - val_loss: 8.2584 - val_acc: 0.2928\n",
      "Epoch 5/25\n",
      "102/102 [==============================] - 50s 486ms/step - loss: 7.2100 - acc: 0.6479 - val_loss: 8.1471 - val_acc: 0.2784\n",
      "Epoch 6/25\n",
      "102/102 [==============================] - 49s 479ms/step - loss: 7.1166 - acc: 0.6695 - val_loss: 7.9632 - val_acc: 0.2847\n",
      "Epoch 7/25\n",
      "102/102 [==============================] - 49s 478ms/step - loss: 7.0929 - acc: 0.6743 - val_loss: 7.7808 - val_acc: 0.3252\n",
      "Epoch 8/25\n",
      "102/102 [==============================] - 49s 479ms/step - loss: 6.9963 - acc: 0.7065 - val_loss: 7.7015 - val_acc: 0.3657\n",
      "Epoch 9/25\n",
      "102/102 [==============================] - 49s 477ms/step - loss: 6.9349 - acc: 0.7115 - val_loss: 7.6099 - val_acc: 0.3970\n",
      "Epoch 10/25\n",
      "102/102 [==============================] - 49s 483ms/step - loss: 6.9032 - acc: 0.7154 - val_loss: 7.5790 - val_acc: 0.3958\n",
      "Epoch 11/25\n",
      "102/102 [==============================] - 49s 477ms/step - loss: 6.8376 - acc: 0.7368 - val_loss: 7.5392 - val_acc: 0.4115\n",
      "Epoch 12/25\n",
      "102/102 [==============================] - 49s 477ms/step - loss: 6.8036 - acc: 0.7377 - val_loss: 7.4909 - val_acc: 0.4219\n",
      "Epoch 13/25\n",
      "102/102 [==============================] - 49s 483ms/step - loss: 6.8058 - acc: 0.7363 - val_loss: 7.4668 - val_acc: 0.4282\n",
      "Epoch 14/25\n",
      "102/102 [==============================] - 51s 504ms/step - loss: 6.7357 - acc: 0.7503 - val_loss: 7.4561 - val_acc: 0.4109\n",
      "Epoch 15/25\n",
      "102/102 [==============================] - 50s 487ms/step - loss: 6.6968 - acc: 0.7626 - val_loss: 7.4145 - val_acc: 0.4294\n",
      "Epoch 16/25\n",
      "102/102 [==============================] - 49s 481ms/step - loss: 6.6822 - acc: 0.7585 - val_loss: 7.4244 - val_acc: 0.4207\n",
      "Epoch 17/25\n",
      "102/102 [==============================] - 50s 486ms/step - loss: 6.6381 - acc: 0.7672 - val_loss: 7.3589 - val_acc: 0.4392\n",
      "Epoch 18/25\n",
      "102/102 [==============================] - 49s 483ms/step - loss: 6.5900 - acc: 0.7804 - val_loss: 7.3332 - val_acc: 0.4508\n",
      "Epoch 19/25\n",
      "102/102 [==============================] - 49s 480ms/step - loss: 6.5704 - acc: 0.7826 - val_loss: 7.3116 - val_acc: 0.4381\n",
      "Epoch 20/25\n",
      "102/102 [==============================] - 50s 488ms/step - loss: 6.5305 - acc: 0.7897 - val_loss: 7.2974 - val_acc: 0.4514\n",
      "Epoch 21/25\n",
      "102/102 [==============================] - 50s 488ms/step - loss: 6.5096 - acc: 0.7933 - val_loss: 7.2633 - val_acc: 0.4549\n",
      "Epoch 22/25\n",
      "102/102 [==============================] - 50s 486ms/step - loss: 6.4519 - acc: 0.8072 - val_loss: 7.2598 - val_acc: 0.4456\n",
      "Epoch 23/25\n",
      "102/102 [==============================] - 49s 481ms/step - loss: 6.4753 - acc: 0.7965 - val_loss: 7.2618 - val_acc: 0.4381\n",
      "Epoch 24/25\n",
      "102/102 [==============================] - 49s 479ms/step - loss: 6.4329 - acc: 0.8011 - val_loss: 7.2300 - val_acc: 0.4514\n",
      "Epoch 25/25\n",
      "102/102 [==============================] - 49s 483ms/step - loss: 6.4038 - acc: 0.8057 - val_loss: 7.2065 - val_acc: 0.4537\n"
     ]
    }
   ],
   "source": [
    "#Preforming the training by using fit \n",
    "# IMPORTANT NOTE: This will take a few minutes!\n",
    "h = NNet.fit(x=BaseX_train, y=BaseY_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0, validation_data = (BaseX_val, BaseY_val), shuffle=True)\n",
    "#NNet.save(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# NNet.load_weights('Weights_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 39ms/step - loss: 7.7583 - acc: 0.3543\n",
      "test loss, test acc: [7.758265972137451, 0.35428571701049805]\n"
     ]
    }
   ],
   "source": [
    "results = NNet.evaluate(X_test,Y_test)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 2:***</span> *Number of filters* \n",
    "\n",
    "Rebuild the function `get_net` to have as an input argument a list of number of filters in each layers, i.e. for the CNN defined above the input should have been `[64, 128, 128, 256, 256]`. Now train the model with the number of filters reduced by half. What were the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_2 (Permute)          (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1 (Conv2D)            (None, 32, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 16, 16, 64)        64        \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 16, 16, 64)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "Conv2D_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 8, 8, 128)         32        \n",
      "_________________________________________________________________\n",
      "Conv2D_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 8, 8, 128)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "FCN_1 (Dense)                (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "FCN_2 (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "FCN_3 (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,392,772\n",
      "Trainable params: 1,392,612\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "102/102 [==============================] - 25s 237ms/step - loss: 5.0660 - acc: 0.3477 - val_loss: 4.6854 - val_acc: 0.2500\n",
      "Epoch 2/25\n",
      "102/102 [==============================] - 24s 231ms/step - loss: 4.5955 - acc: 0.4770 - val_loss: 4.8079 - val_acc: 0.2500\n",
      "Epoch 3/25\n",
      "102/102 [==============================] - 23s 228ms/step - loss: 4.4849 - acc: 0.5057 - val_loss: 4.8478 - val_acc: 0.2338\n",
      "Epoch 4/25\n",
      "102/102 [==============================] - 23s 230ms/step - loss: 4.3832 - acc: 0.5366 - val_loss: 4.8472 - val_acc: 0.2488\n",
      "Epoch 5/25\n",
      "102/102 [==============================] - 23s 226ms/step - loss: 4.3121 - acc: 0.5619 - val_loss: 4.7999 - val_acc: 0.2494\n",
      "Epoch 6/25\n",
      "102/102 [==============================] - 23s 229ms/step - loss: 4.2472 - acc: 0.5891 - val_loss: 4.7758 - val_acc: 0.2535\n",
      "Epoch 7/25\n",
      "102/102 [==============================] - 23s 225ms/step - loss: 4.1964 - acc: 0.5903 - val_loss: 4.7710 - val_acc: 0.2737\n",
      "Epoch 8/25\n",
      "102/102 [==============================] - 23s 225ms/step - loss: 4.1579 - acc: 0.6199 - val_loss: 4.7489 - val_acc: 0.2911\n",
      "Epoch 9/25\n",
      "102/102 [==============================] - 24s 233ms/step - loss: 4.0791 - acc: 0.6396 - val_loss: 4.7192 - val_acc: 0.2998\n",
      "Epoch 10/25\n",
      "102/102 [==============================] - 23s 226ms/step - loss: 4.0570 - acc: 0.6389 - val_loss: 4.7479 - val_acc: 0.2963\n",
      "Epoch 11/25\n",
      "102/102 [==============================] - 23s 228ms/step - loss: 4.0283 - acc: 0.6445 - val_loss: 4.7389 - val_acc: 0.3015\n",
      "Epoch 12/25\n",
      "102/102 [==============================] - 23s 226ms/step - loss: 4.0089 - acc: 0.6570 - val_loss: 4.7611 - val_acc: 0.2940\n",
      "Epoch 13/25\n",
      "102/102 [==============================] - 23s 229ms/step - loss: 3.9895 - acc: 0.6520 - val_loss: 4.7526 - val_acc: 0.2940\n",
      "Epoch 14/25\n",
      "102/102 [==============================] - 24s 234ms/step - loss: 3.9509 - acc: 0.6814 - val_loss: 4.7726 - val_acc: 0.2876\n",
      "Epoch 15/25\n",
      "102/102 [==============================] - 23s 226ms/step - loss: 3.9236 - acc: 0.6851 - val_loss: 4.7543 - val_acc: 0.2917\n",
      "Epoch 16/25\n",
      "102/102 [==============================] - 23s 230ms/step - loss: 3.8979 - acc: 0.6902 - val_loss: 4.7534 - val_acc: 0.2917\n",
      "Epoch 17/25\n",
      "102/102 [==============================] - 23s 226ms/step - loss: 3.9113 - acc: 0.6826 - val_loss: 4.7572 - val_acc: 0.2905\n",
      "Epoch 18/25\n",
      "102/102 [==============================] - 23s 226ms/step - loss: 3.8597 - acc: 0.6958 - val_loss: 4.7619 - val_acc: 0.2922\n",
      "Epoch 19/25\n",
      "102/102 [==============================] - 24s 233ms/step - loss: 3.8542 - acc: 0.6981 - val_loss: 4.7565 - val_acc: 0.2934\n",
      "Epoch 20/25\n",
      "102/102 [==============================] - 23s 228ms/step - loss: 3.8503 - acc: 0.7020 - val_loss: 4.7454 - val_acc: 0.2969\n",
      "Epoch 21/25\n",
      "102/102 [==============================] - 23s 229ms/step - loss: 3.8170 - acc: 0.7127 - val_loss: 4.7639 - val_acc: 0.2946\n",
      "Epoch 22/25\n",
      "102/102 [==============================] - 23s 228ms/step - loss: 3.8001 - acc: 0.7244 - val_loss: 4.7556 - val_acc: 0.2951\n",
      "Epoch 23/25\n",
      "102/102 [==============================] - 23s 229ms/step - loss: 3.7975 - acc: 0.7144 - val_loss: 4.7382 - val_acc: 0.2998\n",
      "Epoch 24/25\n",
      "102/102 [==============================] - 24s 231ms/step - loss: 3.7940 - acc: 0.7192 - val_loss: 4.7445 - val_acc: 0.2980\n",
      "Epoch 25/25\n",
      "102/102 [==============================] - 23s 229ms/step - loss: 3.7567 - acc: 0.7268 - val_loss: 4.7574 - val_acc: 0.2986\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 5.1231 - acc: 0.2457\n",
      "test loss, test acc: [5.123109817504883, 0.24571429193019867]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "def get_net1(input_shape,drop,dropRate,reg,num_filters):\n",
    "    #Defining the network architecture:\n",
    "    model = Sequential()\n",
    "    model.add(Permute((1,2,3),input_shape = input_shape))\n",
    "    model.add(Conv2D(filters=num_filters[0], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_1',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=num_filters[1], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_2',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:    \n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=num_filters[2], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_3',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=num_filters[3], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_4',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=num_filters[4], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_5',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    #Fully connected network tail:      \n",
    "    model.add(Dense(512, activation='elu',name='FCN_1')) \n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(Dense(128, activation='elu',name='FCN_2'))\n",
    "    model.add(Dense(4, activation= 'softmax',name='FCN_3'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "num_filters = [32, 64, 64, 128, 128]\n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 1e-03\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "drop = True\n",
    "dropRate = 0.3\n",
    "reg = 1e-2\n",
    "NNet_new = get_net1(input_shape,drop,dropRate,reg,num_filters)\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "\n",
    "#Compile the network: \n",
    "NNet_new.compile(optimizer=AdamOpt, metrics=['acc'], loss='categorical_crossentropy')\n",
    "\n",
    "\n",
    "h = NNet_new.fit(x=BaseX_train, y=BaseY_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0, validation_data = (BaseX_val, BaseY_val), shuffle=True)\n",
    "results = NNet_new.evaluate(X_test,Y_test)\n",
    "print('test loss, test acc:', results)\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks! See you :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
