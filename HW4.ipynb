{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW: X-ray images classification\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, open Mobaxterm and connect to triton with the user and password you were give with. Activate the environment `2ndPaper` and then type the command `pip install scikit-image`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will be dealing with classification of 32X32 X-ray images of the chest. The image can be classified into one of four options: lungs (l), clavicles (c), and heart (h) and background (b). Even though those labels are dependent, we will treat this task as multiclass and not as multilabel. The dataset for this assignment is located on a shared folder on triton (`/MLdata/MLcourse/X_ray/'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, MaxPool2D, Conv2D, Dropout\n",
    "from tensorflow.keras.layers import Flatten, InputLayer\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import *\n",
    "from skimage.io import imread\n",
    "\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "# %matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto(gpu_options =\n",
    "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "# device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(datapath):\n",
    "    # This part reads the images\n",
    "    classes = ['b','c','l','h']\n",
    "    imagelist = [fn for fn in os.listdir(datapath)]\n",
    "    N = len(imagelist)\n",
    "    num_classes = len(classes)\n",
    "    images = np.zeros((N, 32, 32, 1))\n",
    "    Y = np.zeros((N,num_classes))\n",
    "    ii=0\n",
    "    for fn in imagelist:\n",
    "\n",
    "        src = imread(os.path.join(datapath, fn),1)\n",
    "        img = resize(src,(32,32),order = 3)\n",
    "        \n",
    "        images[ii,:,:,0] = img\n",
    "        cc = -1\n",
    "        for cl in range(len(classes)):\n",
    "            if fn[-5] == classes[cl]:\n",
    "                cc = cl\n",
    "        Y[ii,cc]=1\n",
    "        ii += 1\n",
    "\n",
    "    BaseImages = images\n",
    "    BaseY = Y\n",
    "    return BaseImages, BaseY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_and_val(datapath):\n",
    "    # This part reads the images\n",
    "    classes = ['b','c','l','h']\n",
    "    imagelist = [fn for fn in os.listdir(datapath)]\n",
    "    N = len(imagelist)\n",
    "    num_classes = len(classes)\n",
    "    images = np.zeros((N, 32, 32, 1))\n",
    "    Y = np.zeros((N,num_classes))\n",
    "    ii=0\n",
    "    for fn in imagelist:\n",
    "\n",
    "        images[ii,:,:,0] = imread(os.path.join(datapath, fn),1)\n",
    "        cc = -1\n",
    "        for cl in range(len(classes)):\n",
    "            if fn[-5] == classes[cl]:\n",
    "                cc = cl\n",
    "        Y[ii,cc]=1\n",
    "        ii += 1\n",
    "\n",
    "    return images, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data for training and validation:\n",
    "src_data = 'X_ray/' #/MLdata/MLcourse/\n",
    "train_path = src_data + 'train'\n",
    "val_path = src_data + 'validation'\n",
    "test_path = src_data + 'test'\n",
    "BaseX_train , BaseY_train = preprocess_train_and_val(train_path)\n",
    "BaseX_val , BaseY_val = preprocess_train_and_val(val_path)\n",
    "X_test, Y_test = preprocess(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1: Fully connected layers \n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 1:***</span> *NN with fully connected layers. \n",
    "\n",
    "Elaborate a NN with 2 hidden fully connected layers with 300, 150 neurons and 4 neurons for classification. Use ReLU activation functions for the hidden layers and He_normal for initialization. Don't forget to flatten your image before feedforward to the first dense layer. Name the model `model_relu`.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "n_hidden_start=300\n",
    "model_relu = Sequential()\n",
    "model_relu.add(Flatten(input_shape=(32,32,)))\n",
    "model_relu.add(Dense(n_hidden_start,kernel_initializer=\"HeNormal\",activation='relu'))\n",
    "model_relu.add(Dense(int(n_hidden_start/2),kernel_initializer=\"HeNormal\", activation='relu'))\n",
    "model_relu.add(Dense(int(n_hidden_start/75),kernel_initializer=\"HeNormal\", activation='sigmoid'))\n",
    "\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               307500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 604       \n",
      "=================================================================\n",
      "Total params: 353,254\n",
      "Trainable params: 353,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_relu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: \n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "\n",
    "#Define your optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model with the optimizer above, accuracy metric and adequate loss for multiclass task. Train your model on the training set and evaluate the model on the testing set. Print the accuracy and loss over the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: results/init_weigths_relu\\assets\n",
      "Epoch 1/25\n",
      "102/102 [==============================] - 2s 13ms/step - loss: 1.3691 - accuracy: 0.3108 - val_loss: 1.1685 - val_accuracy: 0.5584\n",
      "Epoch 2/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.1392 - accuracy: 0.5935 - val_loss: 1.0448 - val_accuracy: 0.6499\n",
      "Epoch 3/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0370 - accuracy: 0.6620 - val_loss: 0.9682 - val_accuracy: 0.6840\n",
      "Epoch 4/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.9658 - accuracy: 0.7067 - val_loss: 0.9167 - val_accuracy: 0.7199\n",
      "Epoch 5/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.9086 - accuracy: 0.7216 - val_loss: 0.8743 - val_accuracy: 0.7384\n",
      "Epoch 6/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.8642 - accuracy: 0.7481 - val_loss: 0.8394 - val_accuracy: 0.7448\n",
      "Epoch 7/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.8341 - accuracy: 0.7618 - val_loss: 0.8113 - val_accuracy: 0.7483\n",
      "Epoch 8/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7972 - accuracy: 0.7626 - val_loss: 0.7858 - val_accuracy: 0.7662\n",
      "Epoch 9/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7659 - accuracy: 0.7792 - val_loss: 0.7634 - val_accuracy: 0.7650\n",
      "Epoch 10/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7569 - accuracy: 0.7784 - val_loss: 0.7415 - val_accuracy: 0.7708\n",
      "Epoch 11/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7204 - accuracy: 0.7884 - val_loss: 0.7218 - val_accuracy: 0.7708\n",
      "Epoch 12/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.7068 - accuracy: 0.8008 - val_loss: 0.7012 - val_accuracy: 0.7824\n",
      "Epoch 13/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.6766 - accuracy: 0.8027 - val_loss: 0.6848 - val_accuracy: 0.7847\n",
      "Epoch 14/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.6683 - accuracy: 0.8005 - val_loss: 0.6694 - val_accuracy: 0.7905\n",
      "Epoch 15/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.6578 - accuracy: 0.7981 - val_loss: 0.6545 - val_accuracy: 0.7934\n",
      "Epoch 16/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.8110 - val_loss: 0.6410 - val_accuracy: 0.8027\n",
      "Epoch 17/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.6261 - accuracy: 0.8090 - val_loss: 0.6272 - val_accuracy: 0.8044\n",
      "Epoch 18/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.6154 - accuracy: 0.8162 - val_loss: 0.6143 - val_accuracy: 0.8061\n",
      "Epoch 19/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.6096 - accuracy: 0.8138 - val_loss: 0.6021 - val_accuracy: 0.8102\n",
      "Epoch 20/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.5883 - accuracy: 0.8236 - val_loss: 0.5942 - val_accuracy: 0.8113\n",
      "Epoch 21/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.5858 - accuracy: 0.8179 - val_loss: 0.5808 - val_accuracy: 0.8148\n",
      "Epoch 22/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.5709 - accuracy: 0.8295 - val_loss: 0.5708 - val_accuracy: 0.8206\n",
      "Epoch 23/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.5653 - accuracy: 0.8274 - val_loss: 0.5622 - val_accuracy: 0.8200\n",
      "Epoch 24/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.8444 - val_loss: 0.5497 - val_accuracy: 0.8247\n",
      "Epoch 25/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.5292 - accuracy: 0.8345 - val_loss: 0.5441 - val_accuracy: 0.8241\n",
      "Evaluate on test data\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7987 - accuracy: 0.6629\n",
      "test loss, test acc: [0.7987362146377563, 0.6628571152687073]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "model_relu.compile(optimizer=AdamOpt,metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "\n",
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"init_weigths_relu\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model_relu.save(model_path)\n",
    "\n",
    "history_relu = model_relu.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs,validation_data=(BaseX_val,BaseY_val))\n",
    "\n",
    "y_pred_test = model_relu.predict(X_test)\n",
    "print(\"Evaluate on test data\")\n",
    "results = model_relu.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 2:***</span> *Activation functions.* \n",
    "\n",
    "Change the activation functions to LeakyRelu or tanh or sigmoid. Name the new model `new_a_model`. Explain how it can affect the model.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "n_hidden_start=300\n",
    "new_a_model = Sequential()\n",
    "new_a_model.add(Flatten(input_shape=(32,32,)))\n",
    "new_a_model.add(Dense(n_hidden_start,kernel_initializer=\"HeNormal\",activation='sigmoid'))\n",
    "new_a_model.add(Dense(int(n_hidden_start/2),kernel_initializer=\"HeNormal\", activation='sigmoid'))\n",
    "new_a_model.add(Dense(int(n_hidden_start/75),kernel_initializer=\"HeNormal\", activation='sigmoid'))\n",
    "\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               307500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 604       \n",
      "=================================================================\n",
      "Total params: 353,254\n",
      "Trainable params: 353,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_a_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 3:***</span> *Number of epochs.* \n",
    "\n",
    "Train the new model using 25 and 40 epochs. What difference does it makes in term of performance? Remember to save the compiled model for having initialized weights for every run as we did in tutorial 12. Evaluate each trained model on the test set*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: \n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: results/init_weigths_newmodel\\assets\n",
      "Epoch 1/25\n",
      "102/102 [==============================] - 1s 4ms/step - loss: 1.5316 - accuracy: 0.2412 - val_loss: 1.4317 - val_accuracy: 0.2500\n",
      "Epoch 2/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.4146 - accuracy: 0.2596 - val_loss: 1.3827 - val_accuracy: 0.3501\n",
      "Epoch 3/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.3743 - accuracy: 0.3717 - val_loss: 1.3566 - val_accuracy: 0.4381\n",
      "Epoch 4/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.3516 - accuracy: 0.4544 - val_loss: 1.3324 - val_accuracy: 0.4826\n",
      "Epoch 5/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.3277 - accuracy: 0.4853 - val_loss: 1.3090 - val_accuracy: 0.5041\n",
      "Epoch 6/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.3048 - accuracy: 0.4998 - val_loss: 1.2862 - val_accuracy: 0.5156\n",
      "Epoch 7/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.2856 - accuracy: 0.5209 - val_loss: 1.2638 - val_accuracy: 0.5353\n",
      "Epoch 8/25\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 1.2596 - accuracy: 0.5442 - val_loss: 1.2422 - val_accuracy: 0.5475\n",
      "Epoch 9/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.2399 - accuracy: 0.5516 - val_loss: 1.2216 - val_accuracy: 0.5550\n",
      "Epoch 10/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.2193 - accuracy: 0.5606 - val_loss: 1.2018 - val_accuracy: 0.5729\n",
      "Epoch 11/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.2026 - accuracy: 0.5589 - val_loss: 1.1829 - val_accuracy: 0.5793\n",
      "Epoch 12/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.1839 - accuracy: 0.5682 - val_loss: 1.1655 - val_accuracy: 0.5747\n",
      "Epoch 13/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.1574 - accuracy: 0.5931 - val_loss: 1.1493 - val_accuracy: 0.5856\n",
      "Epoch 14/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.1498 - accuracy: 0.5860 - val_loss: 1.1342 - val_accuracy: 0.5949\n",
      "Epoch 15/25\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 1.1326 - accuracy: 0.5901 - val_loss: 1.1199 - val_accuracy: 0.6013\n",
      "Epoch 16/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.1210 - accuracy: 0.6031 - val_loss: 1.1066 - val_accuracy: 0.6117\n",
      "Epoch 17/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.1028 - accuracy: 0.6068 - val_loss: 1.0943 - val_accuracy: 0.6181\n",
      "Epoch 18/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0880 - accuracy: 0.6179 - val_loss: 1.0826 - val_accuracy: 0.6215\n",
      "Epoch 19/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0870 - accuracy: 0.5993 - val_loss: 1.0719 - val_accuracy: 0.6140\n",
      "Epoch 20/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0787 - accuracy: 0.6055 - val_loss: 1.0616 - val_accuracy: 0.6244\n",
      "Epoch 21/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0608 - accuracy: 0.6200 - val_loss: 1.0519 - val_accuracy: 0.6319\n",
      "Epoch 22/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0568 - accuracy: 0.6179 - val_loss: 1.0429 - val_accuracy: 0.6383\n",
      "Epoch 23/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0430 - accuracy: 0.6156 - val_loss: 1.0344 - val_accuracy: 0.6418\n",
      "Epoch 24/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0462 - accuracy: 0.6240 - val_loss: 1.0263 - val_accuracy: 0.6360\n",
      "Epoch 25/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0379 - accuracy: 0.6152 - val_loss: 1.0187 - val_accuracy: 0.6406\n",
      "Evaluate on test data\n",
      "3/3 [==============================] - 0s 0s/step - loss: 1.0651 - accuracy: 0.5657\n",
      "test loss, test acc: [1.065070390701294, 0.5657142996788025]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "new_a_model.compile(optimizer=AdamOpt,metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "# Code taken from TUTORIAL 11:::::\n",
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"init_weigths_newmodel\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "new_a_model.save(model_path)\n",
    "# print('Saved initialized model at %s ' % model_path)\n",
    "\n",
    "history_model_a = new_a_model.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs,validation_data = (BaseX_val,BaseY_val))\n",
    "\n",
    "y_pred_test = new_a_model.predict(X_test)\n",
    "print(\"Evaluate on test data\")\n",
    "results = new_a_model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: \n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "batch_size = 64\n",
    "epochs = 40\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "102/102 [==============================] - 1s 4ms/step - loss: 1.4983 - accuracy: 0.2447 - val_loss: 1.4335 - val_accuracy: 0.2315\n",
      "Epoch 2/40\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 1.4053 - accuracy: 0.2831 - val_loss: 1.3829 - val_accuracy: 0.3484\n",
      "Epoch 3/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.3698 - accuracy: 0.4032 - val_loss: 1.3562 - val_accuracy: 0.4427\n",
      "Epoch 4/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.3448 - accuracy: 0.4699 - val_loss: 1.3317 - val_accuracy: 0.4745\n",
      "Epoch 5/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.3216 - accuracy: 0.4898 - val_loss: 1.3084 - val_accuracy: 0.5012\n",
      "Epoch 6/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.2990 - accuracy: 0.5139 - val_loss: 1.2857 - val_accuracy: 0.5255\n",
      "Epoch 7/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.2771 - accuracy: 0.5270 - val_loss: 1.2633 - val_accuracy: 0.5376\n",
      "Epoch 8/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.2560 - accuracy: 0.5383 - val_loss: 1.2421 - val_accuracy: 0.5561\n",
      "Epoch 9/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.2353 - accuracy: 0.5494 - val_loss: 1.2213 - val_accuracy: 0.5631\n",
      "Epoch 10/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.2154 - accuracy: 0.5639 - val_loss: 1.2015 - val_accuracy: 0.5654\n",
      "Epoch 11/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.1969 - accuracy: 0.5752 - val_loss: 1.1829 - val_accuracy: 0.5758\n",
      "Epoch 12/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.1794 - accuracy: 0.5748 - val_loss: 1.1656 - val_accuracy: 0.5787\n",
      "Epoch 13/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.1628 - accuracy: 0.5904 - val_loss: 1.1495 - val_accuracy: 0.5851\n",
      "Epoch 14/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.1473 - accuracy: 0.5797 - val_loss: 1.1341 - val_accuracy: 0.5995\n",
      "Epoch 15/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.1329 - accuracy: 0.5958 - val_loss: 1.1200 - val_accuracy: 0.6001\n",
      "Epoch 16/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.1193 - accuracy: 0.5927 - val_loss: 1.1069 - val_accuracy: 0.6111\n",
      "Epoch 17/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.1067 - accuracy: 0.6066 - val_loss: 1.0945 - val_accuracy: 0.6100\n",
      "Epoch 18/40\n",
      "102/102 [==============================] - 0s 3ms/step - loss: 1.0946 - accuracy: 0.5999 - val_loss: 1.0833 - val_accuracy: 0.6244\n",
      "Epoch 19/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0833 - accuracy: 0.6094 - val_loss: 1.0719 - val_accuracy: 0.6209\n",
      "Epoch 20/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0729 - accuracy: 0.6074 - val_loss: 1.0616 - val_accuracy: 0.6267\n",
      "Epoch 21/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0626 - accuracy: 0.6175 - val_loss: 1.0520 - val_accuracy: 0.6221\n",
      "Epoch 22/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0533 - accuracy: 0.6117 - val_loss: 1.0429 - val_accuracy: 0.6389\n",
      "Epoch 23/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0442 - accuracy: 0.6196 - val_loss: 1.0342 - val_accuracy: 0.6447\n",
      "Epoch 24/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0355 - accuracy: 0.6248 - val_loss: 1.0258 - val_accuracy: 0.6389\n",
      "Epoch 25/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0275 - accuracy: 0.6213 - val_loss: 1.0183 - val_accuracy: 0.6481\n",
      "Epoch 26/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0197 - accuracy: 0.6307 - val_loss: 1.0110 - val_accuracy: 0.6470\n",
      "Epoch 27/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0124 - accuracy: 0.6342 - val_loss: 1.0036 - val_accuracy: 0.6539\n",
      "Epoch 28/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 1.0052 - accuracy: 0.6285 - val_loss: 0.9970 - val_accuracy: 0.6505\n",
      "Epoch 29/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.9983 - accuracy: 0.6375 - val_loss: 0.9905 - val_accuracy: 0.6562\n",
      "Epoch 30/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.9917 - accuracy: 0.6407 - val_loss: 0.9840 - val_accuracy: 0.6586\n",
      "Epoch 31/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.9853 - accuracy: 0.6426 - val_loss: 0.9781 - val_accuracy: 0.6574\n",
      "Epoch 32/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.9796 - accuracy: 0.6461 - val_loss: 0.9723 - val_accuracy: 0.6603\n",
      "Epoch 33/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.9735 - accuracy: 0.6461 - val_loss: 0.9668 - val_accuracy: 0.6649\n",
      "Epoch 34/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.9675 - accuracy: 0.6484 - val_loss: 0.9613 - val_accuracy: 0.6649\n",
      "Epoch 35/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.9623 - accuracy: 0.6504 - val_loss: 0.9561 - val_accuracy: 0.6638\n",
      "Epoch 36/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.9568 - accuracy: 0.6528 - val_loss: 0.9509 - val_accuracy: 0.6690\n",
      "Epoch 37/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.9512 - accuracy: 0.6568 - val_loss: 0.9461 - val_accuracy: 0.6736\n",
      "Epoch 38/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.9466 - accuracy: 0.6580 - val_loss: 0.9410 - val_accuracy: 0.6701\n",
      "Epoch 39/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.9415 - accuracy: 0.6591 - val_loss: 0.9372 - val_accuracy: 0.6667\n",
      "Epoch 40/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 0.9364 - accuracy: 0.6620 - val_loss: 0.9318 - val_accuracy: 0.6707\n",
      "Evaluate on test data\n",
      "3/3 [==============================] - 0s 0s/step - loss: 0.9781 - accuracy: 0.6114\n",
      "test loss, test acc: [0.9780610203742981, 0.6114285588264465]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "new_a_model.compile(optimizer=AdamOpt,metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "\n",
    "new_a_model = tf.keras.models.load_model(\"results/init_weigths_newmodel\") # Initializing weights before total run\n",
    "\n",
    "history_model_a = new_a_model.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs,validation_data=(BaseX_val,BaseY_val))\n",
    "\n",
    "y_pred_test = new_a_model.predict(X_test)\n",
    "print(\"Evaluate on test data\")\n",
    "results = new_a_model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "print(\"test loss, test acc:\", results)\n",
    "#-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 4:***</span> *Mini-batches.* \n",
    "\n",
    "Build the `model_relu` again and run it with a batch size of 32 instead of 64. What are the advantages of the mini-batch vs. SGD?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "n_hidden_start=300\n",
    "model_relu = Sequential()\n",
    "model_relu.add(Flatten(input_shape=(32,32,)))\n",
    "model_relu.add(Dense(n_hidden_start,kernel_initializer=\"HeNormal\", activation='relu'))\n",
    "model_relu.add(Dense(int(n_hidden_start/2),kernel_initializer=\"HeNormal\", activation='relu'))\n",
    "model_relu.add(Dense(int(n_hidden_start/75),kernel_initializer=\"HeNormal\", activation='sigmoid'))\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "#Define your optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "203/203 [==============================] - 1s 2ms/step - loss: 1.2326 - accuracy: 0.4765 - val_loss: 1.0844 - val_accuracy: 0.6262\n",
      "Epoch 2/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.0253 - accuracy: 0.6645 - val_loss: 0.9594 - val_accuracy: 0.6973\n",
      "Epoch 3/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.9248 - accuracy: 0.7169 - val_loss: 0.8847 - val_accuracy: 0.7251\n",
      "Epoch 4/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8584 - accuracy: 0.7487 - val_loss: 0.8307 - val_accuracy: 0.7431\n",
      "Epoch 5/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8085 - accuracy: 0.7600 - val_loss: 0.7906 - val_accuracy: 0.7587\n",
      "Epoch 6/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.7690 - accuracy: 0.7740 - val_loss: 0.7588 - val_accuracy: 0.7662\n",
      "Epoch 7/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.7345 - accuracy: 0.7847 - val_loss: 0.7264 - val_accuracy: 0.7743\n",
      "Epoch 8/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.7030 - accuracy: 0.7909 - val_loss: 0.6978 - val_accuracy: 0.7859\n",
      "Epoch 9/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.6756 - accuracy: 0.8003 - val_loss: 0.6773 - val_accuracy: 0.7853\n",
      "Epoch 10/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.6517 - accuracy: 0.8054 - val_loss: 0.6523 - val_accuracy: 0.7957\n",
      "Epoch 11/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.6294 - accuracy: 0.8133 - val_loss: 0.6320 - val_accuracy: 0.8038\n",
      "Epoch 12/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.6097 - accuracy: 0.8182 - val_loss: 0.6173 - val_accuracy: 0.7986\n",
      "Epoch 13/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.5903 - accuracy: 0.8224 - val_loss: 0.5949 - val_accuracy: 0.8125\n",
      "Epoch 14/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.5721 - accuracy: 0.8255 - val_loss: 0.5817 - val_accuracy: 0.8166\n",
      "Epoch 15/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.5558 - accuracy: 0.8313 - val_loss: 0.5655 - val_accuracy: 0.8223\n",
      "Epoch 16/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.5417 - accuracy: 0.8355 - val_loss: 0.5548 - val_accuracy: 0.8212\n",
      "Epoch 17/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.5278 - accuracy: 0.8394 - val_loss: 0.5382 - val_accuracy: 0.8229\n",
      "Epoch 18/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.5148 - accuracy: 0.8446 - val_loss: 0.5264 - val_accuracy: 0.8281\n",
      "Epoch 19/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.8503 - val_loss: 0.5163 - val_accuracy: 0.8333\n",
      "Epoch 20/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.8511 - val_loss: 0.5050 - val_accuracy: 0.8351\n",
      "Epoch 21/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.8548 - val_loss: 0.4992 - val_accuracy: 0.8391\n",
      "Epoch 22/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.4702 - accuracy: 0.8577 - val_loss: 0.4871 - val_accuracy: 0.8443\n",
      "Epoch 23/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.4598 - accuracy: 0.8618 - val_loss: 0.4806 - val_accuracy: 0.8490\n",
      "Epoch 24/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.4517 - accuracy: 0.8639 - val_loss: 0.4703 - val_accuracy: 0.8495\n",
      "Epoch 25/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.4425 - accuracy: 0.8644 - val_loss: 0.4647 - val_accuracy: 0.8571\n",
      "Epoch 26/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.4347 - accuracy: 0.8698 - val_loss: 0.4569 - val_accuracy: 0.8588\n",
      "Epoch 27/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.4274 - accuracy: 0.8710 - val_loss: 0.4493 - val_accuracy: 0.8571\n",
      "Epoch 28/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.4204 - accuracy: 0.8741 - val_loss: 0.4430 - val_accuracy: 0.8634\n",
      "Epoch 29/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.4128 - accuracy: 0.8760 - val_loss: 0.4380 - val_accuracy: 0.8605\n",
      "Epoch 30/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.4058 - accuracy: 0.8778 - val_loss: 0.4304 - val_accuracy: 0.8634\n",
      "Epoch 31/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.8791 - val_loss: 0.4274 - val_accuracy: 0.8634\n",
      "Epoch 32/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8784 - val_loss: 0.4198 - val_accuracy: 0.8681\n",
      "Epoch 33/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3876 - accuracy: 0.8812 - val_loss: 0.4145 - val_accuracy: 0.8692\n",
      "Epoch 34/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3824 - accuracy: 0.8848 - val_loss: 0.4103 - val_accuracy: 0.8727\n",
      "Epoch 35/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3773 - accuracy: 0.8848 - val_loss: 0.4044 - val_accuracy: 0.8750\n",
      "Epoch 36/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3720 - accuracy: 0.8868 - val_loss: 0.3996 - val_accuracy: 0.8779\n",
      "Epoch 37/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3667 - accuracy: 0.8877 - val_loss: 0.3962 - val_accuracy: 0.8750\n",
      "Epoch 38/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3623 - accuracy: 0.8865 - val_loss: 0.3918 - val_accuracy: 0.8819\n",
      "Epoch 39/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3577 - accuracy: 0.8899 - val_loss: 0.3881 - val_accuracy: 0.8791\n",
      "Epoch 40/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3531 - accuracy: 0.8911 - val_loss: 0.3854 - val_accuracy: 0.8819\n",
      "Epoch 41/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.8913 - val_loss: 0.3812 - val_accuracy: 0.8843\n",
      "Epoch 42/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3447 - accuracy: 0.8920 - val_loss: 0.3784 - val_accuracy: 0.8825\n",
      "Epoch 43/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3412 - accuracy: 0.8931 - val_loss: 0.3734 - val_accuracy: 0.8866\n",
      "Epoch 44/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.8962 - val_loss: 0.3716 - val_accuracy: 0.8848\n",
      "Epoch 45/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3331 - accuracy: 0.8951 - val_loss: 0.3681 - val_accuracy: 0.8872\n",
      "Epoch 46/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3295 - accuracy: 0.8948 - val_loss: 0.3661 - val_accuracy: 0.8872\n",
      "Epoch 47/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3259 - accuracy: 0.8988 - val_loss: 0.3623 - val_accuracy: 0.8895\n",
      "Epoch 48/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3230 - accuracy: 0.8968 - val_loss: 0.3619 - val_accuracy: 0.8889\n",
      "Epoch 49/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3191 - accuracy: 0.8977 - val_loss: 0.3547 - val_accuracy: 0.8929\n",
      "Epoch 50/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.3164 - accuracy: 0.9015 - val_loss: 0.3539 - val_accuracy: 0.8929\n",
      "Evaluate on test data\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8465 - accuracy: 0.6686\n",
      "test loss, test acc: [0.846523106098175, 0.668571412563324]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "model_relu.compile(optimizer=AdamOpt,metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "\n",
    "model_relu = tf.keras.models.load_model(\"results/init_weigths_relu\")\n",
    "history_relu = model_relu.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs,validation_data=(BaseX_val,BaseY_val))\n",
    "\n",
    "y_pred_test = model_relu.predict(X_test)\n",
    "print(\"Evaluate on test data\")\n",
    "results = model_relu.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "print(\"test loss, test acc:\", results)\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 4:***</span> *Batch normalization.* \n",
    "\n",
    "Build the `new_a_model` again and add batch normalization layers. How does it impact your results?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "n_hidden_start=300\n",
    "new_a_model = Sequential()\n",
    "new_a_model.add(Flatten(input_shape=(32,32,)))\n",
    "new_a_model.add(Dense(n_hidden_start,kernel_initializer=\"HeNormal\",activation='sigmoid'))\n",
    "new_a_model.add(BatchNormalization())\n",
    "new_a_model.add(Dense(int(n_hidden_start/2), kernel_initializer=\"HeNormal\",activation='sigmoid'))\n",
    "new_a_model.add(BatchNormalization())\n",
    "new_a_model.add(Dense(int(n_hidden_start/75), kernel_initializer=\"HeNormal\",activation='sigmoid'))\n",
    "new_a_model.add(BatchNormalization())\n",
    "#---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "#Define your optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "#Compile the network: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "203/203 [==============================] - 1s 3ms/step - loss: 1.4578 - accuracy: 0.2575 - val_loss: 1.3916 - val_accuracy: 0.3438\n",
      "Epoch 2/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.3694 - accuracy: 0.3951 - val_loss: 1.3474 - val_accuracy: 0.4630\n",
      "Epoch 3/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.3295 - accuracy: 0.4869 - val_loss: 1.3093 - val_accuracy: 0.4913\n",
      "Epoch 4/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.2926 - accuracy: 0.5153 - val_loss: 1.2727 - val_accuracy: 0.5214\n",
      "Epoch 5/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.2577 - accuracy: 0.5411 - val_loss: 1.2371 - val_accuracy: 0.5411\n",
      "Epoch 6/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.2245 - accuracy: 0.5598 - val_loss: 1.2047 - val_accuracy: 0.5677\n",
      "Epoch 7/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.1941 - accuracy: 0.5690 - val_loss: 1.1758 - val_accuracy: 0.5856\n",
      "Epoch 8/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.1670 - accuracy: 0.5789 - val_loss: 1.1489 - val_accuracy: 0.5897\n",
      "Epoch 9/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.1424 - accuracy: 0.5888 - val_loss: 1.1259 - val_accuracy: 0.5995\n",
      "Epoch 10/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.1207 - accuracy: 0.5978 - val_loss: 1.1049 - val_accuracy: 0.6024\n",
      "Epoch 11/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.1011 - accuracy: 0.5989 - val_loss: 1.0858 - val_accuracy: 0.6117\n",
      "Epoch 12/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.0832 - accuracy: 0.6074 - val_loss: 1.0689 - val_accuracy: 0.6163\n",
      "Epoch 13/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.0668 - accuracy: 0.6111 - val_loss: 1.0538 - val_accuracy: 0.6192\n",
      "Epoch 14/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.0519 - accuracy: 0.6154 - val_loss: 1.0400 - val_accuracy: 0.6372\n",
      "Epoch 15/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.0385 - accuracy: 0.6222 - val_loss: 1.0265 - val_accuracy: 0.6458\n",
      "Epoch 16/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.0259 - accuracy: 0.6247 - val_loss: 1.0147 - val_accuracy: 0.6453\n",
      "Epoch 17/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.0144 - accuracy: 0.6302 - val_loss: 1.0037 - val_accuracy: 0.6545\n",
      "Epoch 18/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 1.0034 - accuracy: 0.6342 - val_loss: 0.9940 - val_accuracy: 0.6383\n",
      "Epoch 19/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.9936 - accuracy: 0.6358 - val_loss: 0.9841 - val_accuracy: 0.6528\n",
      "Epoch 20/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.9840 - accuracy: 0.6412 - val_loss: 0.9752 - val_accuracy: 0.6580\n",
      "Epoch 21/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.9749 - accuracy: 0.6441 - val_loss: 0.9669 - val_accuracy: 0.6632\n",
      "Epoch 22/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.9663 - accuracy: 0.6477 - val_loss: 0.9586 - val_accuracy: 0.6661\n",
      "Epoch 23/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.9582 - accuracy: 0.6506 - val_loss: 0.9509 - val_accuracy: 0.6649\n",
      "Epoch 24/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.9503 - accuracy: 0.6569 - val_loss: 0.9440 - val_accuracy: 0.6655\n",
      "Epoch 25/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.9424 - accuracy: 0.6593 - val_loss: 0.9375 - val_accuracy: 0.6707\n",
      "Epoch 26/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.9352 - accuracy: 0.6627 - val_loss: 0.9299 - val_accuracy: 0.6806\n",
      "Epoch 27/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.9283 - accuracy: 0.6671 - val_loss: 0.9228 - val_accuracy: 0.6782\n",
      "Epoch 28/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.9214 - accuracy: 0.6701 - val_loss: 0.9163 - val_accuracy: 0.6834\n",
      "Epoch 29/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.9144 - accuracy: 0.6749 - val_loss: 0.9103 - val_accuracy: 0.6800\n",
      "Epoch 30/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.9078 - accuracy: 0.6772 - val_loss: 0.9046 - val_accuracy: 0.6846\n",
      "Epoch 31/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.9015 - accuracy: 0.6798 - val_loss: 0.8983 - val_accuracy: 0.6829\n",
      "Epoch 32/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8952 - accuracy: 0.6800 - val_loss: 0.8924 - val_accuracy: 0.6846\n",
      "Epoch 33/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8889 - accuracy: 0.6835 - val_loss: 0.8870 - val_accuracy: 0.6852\n",
      "Epoch 34/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8830 - accuracy: 0.6843 - val_loss: 0.8811 - val_accuracy: 0.6834\n",
      "Epoch 35/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8761 - accuracy: 0.6849 - val_loss: 0.8781 - val_accuracy: 0.6921\n",
      "Epoch 36/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8714 - accuracy: 0.6891 - val_loss: 0.8700 - val_accuracy: 0.6921\n",
      "Epoch 37/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8656 - accuracy: 0.6918 - val_loss: 0.8649 - val_accuracy: 0.6927\n",
      "Epoch 38/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8599 - accuracy: 0.6926 - val_loss: 0.8602 - val_accuracy: 0.6979\n",
      "Epoch 39/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8538 - accuracy: 0.6977 - val_loss: 0.8558 - val_accuracy: 0.6962\n",
      "Epoch 40/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8490 - accuracy: 0.6973 - val_loss: 0.8502 - val_accuracy: 0.7008\n",
      "Epoch 41/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8435 - accuracy: 0.6982 - val_loss: 0.8448 - val_accuracy: 0.6985\n",
      "Epoch 42/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8384 - accuracy: 0.6986 - val_loss: 0.8404 - val_accuracy: 0.7066\n",
      "Epoch 43/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8334 - accuracy: 0.7023 - val_loss: 0.8354 - val_accuracy: 0.7072\n",
      "Epoch 44/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8282 - accuracy: 0.7061 - val_loss: 0.8311 - val_accuracy: 0.7083\n",
      "Epoch 45/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8233 - accuracy: 0.7059 - val_loss: 0.8271 - val_accuracy: 0.7089\n",
      "Epoch 46/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8184 - accuracy: 0.7065 - val_loss: 0.8221 - val_accuracy: 0.7054\n",
      "Epoch 47/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8136 - accuracy: 0.7084 - val_loss: 0.8177 - val_accuracy: 0.7066\n",
      "Epoch 48/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8090 - accuracy: 0.7101 - val_loss: 0.8136 - val_accuracy: 0.7089\n",
      "Epoch 49/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.8046 - accuracy: 0.7107 - val_loss: 0.8097 - val_accuracy: 0.7106\n",
      "Epoch 50/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 0.7999 - accuracy: 0.7119 - val_loss: 0.8058 - val_accuracy: 0.7130\n",
      "Evaluate on test data\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.8871 - accuracy: 0.6457\n",
      "test loss, test acc: [0.8870556354522705, 0.645714282989502]\n"
     ]
    }
   ],
   "source": [
    "#Preforming the training by using fit \n",
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "new_a_model.compile(optimizer=AdamOpt,metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "new_a_model = tf.keras.models.load_model(\"results/init_weigths_newmodel\") \n",
    "history_model_a = new_a_model.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs,validation_data = (BaseX_val,BaseY_val))\n",
    "\n",
    "y_pred_test = new_a_model.predict(X_test)\n",
    "print(\"Evaluate on test data\")\n",
    "results = new_a_model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "print(\"test loss, test acc:\", results)\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2: Convolutional Neural Network (CNN)\n",
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 1:***</span> *2D CNN.* \n",
    "\n",
    "Have a look at the model below and answer the following:\n",
    "* How many layers does it have?\n",
    "* How many filter in each layer?\n",
    "* Would the number of parmaters be similar to a fully connected NN?\n",
    "* Is this specific NN performing regularization?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(input_shape,drop,dropRate,reg):\n",
    "    #Defining the network architecture:\n",
    "    model = Sequential()\n",
    "    model.add(Permute((1,2,3),input_shape = input_shape))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_1',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_2',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:    \n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_3',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_4',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_5',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    #Fully connected network tail:      \n",
    "    model.add(Dense(512, activation='elu',name='FCN_1')) \n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(Dense(128, activation='elu',name='FCN_2'))\n",
    "    model.add(Dense(4, activation= 'softmax',name='FCN_3'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1 (Conv2D)            (None, 32, 32, 64)        640       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "Conv2D_5 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "FCN_1 (Dense)                (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "FCN_2 (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "FCN_3 (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 3,271,492\n",
      "Trainable params: 3,271,332\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 1e-03\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "drop = True\n",
    "dropRate = 0.3\n",
    "reg = 1e-2\n",
    "NNet = get_net(input_shape,drop,dropRate,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_1 (Permute)          (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1 (Conv2D)            (None, 32, 32, 64)        640       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 64)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "Conv2D_5 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "FCN_1 (Dense)                (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "FCN_2 (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "FCN_3 (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 3,271,492\n",
      "Trainable params: 3,271,332\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NNet=get_net(input_shape,drop,dropRate,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import *\n",
    "import os\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "\n",
    "#Compile the network: \n",
    "NNet.compile(optimizer=AdamOpt, metrics=['acc'], loss='categorical_crossentropy')\n",
    "\n",
    "#Saving checkpoints during training:\n",
    "Checkpath = os.getcwd()\n",
    "Checkp = ModelCheckpoint(Checkpath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, save_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "102/102 [==============================] - 51s 492ms/step - loss: 8.2400 - acc: 0.3780 - val_loss: 8.0294 - val_acc: 0.2500\n",
      "Epoch 2/25\n",
      "102/102 [==============================] - 49s 476ms/step - loss: 7.6110 - acc: 0.5273 - val_loss: 8.1661 - val_acc: 0.2500\n",
      "Epoch 3/25\n",
      "102/102 [==============================] - 49s 484ms/step - loss: 7.4201 - acc: 0.5844 - val_loss: 8.2606 - val_acc: 0.2575\n",
      "Epoch 4/25\n",
      "102/102 [==============================] - 49s 479ms/step - loss: 7.3105 - acc: 0.6202 - val_loss: 8.2584 - val_acc: 0.2928\n",
      "Epoch 5/25\n",
      "102/102 [==============================] - 50s 486ms/step - loss: 7.2100 - acc: 0.6479 - val_loss: 8.1471 - val_acc: 0.2784\n",
      "Epoch 6/25\n",
      "102/102 [==============================] - 49s 479ms/step - loss: 7.1166 - acc: 0.6695 - val_loss: 7.9632 - val_acc: 0.2847\n",
      "Epoch 7/25\n",
      "102/102 [==============================] - 49s 478ms/step - loss: 7.0929 - acc: 0.6743 - val_loss: 7.7808 - val_acc: 0.3252\n",
      "Epoch 8/25\n",
      "102/102 [==============================] - 49s 479ms/step - loss: 6.9963 - acc: 0.7065 - val_loss: 7.7015 - val_acc: 0.3657\n",
      "Epoch 9/25\n",
      "102/102 [==============================] - 49s 477ms/step - loss: 6.9349 - acc: 0.7115 - val_loss: 7.6099 - val_acc: 0.3970\n",
      "Epoch 10/25\n",
      "102/102 [==============================] - 49s 483ms/step - loss: 6.9032 - acc: 0.7154 - val_loss: 7.5790 - val_acc: 0.3958\n",
      "Epoch 11/25\n",
      "102/102 [==============================] - 49s 477ms/step - loss: 6.8376 - acc: 0.7368 - val_loss: 7.5392 - val_acc: 0.4115\n",
      "Epoch 12/25\n",
      "102/102 [==============================] - 49s 477ms/step - loss: 6.8036 - acc: 0.7377 - val_loss: 7.4909 - val_acc: 0.4219\n",
      "Epoch 13/25\n",
      "102/102 [==============================] - 49s 483ms/step - loss: 6.8058 - acc: 0.7363 - val_loss: 7.4668 - val_acc: 0.4282\n",
      "Epoch 14/25\n",
      "102/102 [==============================] - 51s 504ms/step - loss: 6.7357 - acc: 0.7503 - val_loss: 7.4561 - val_acc: 0.4109\n",
      "Epoch 15/25\n",
      "102/102 [==============================] - 50s 487ms/step - loss: 6.6968 - acc: 0.7626 - val_loss: 7.4145 - val_acc: 0.4294\n",
      "Epoch 16/25\n",
      "102/102 [==============================] - 49s 481ms/step - loss: 6.6822 - acc: 0.7585 - val_loss: 7.4244 - val_acc: 0.4207\n",
      "Epoch 17/25\n",
      "102/102 [==============================] - 50s 486ms/step - loss: 6.6381 - acc: 0.7672 - val_loss: 7.3589 - val_acc: 0.4392\n",
      "Epoch 18/25\n",
      "102/102 [==============================] - 49s 483ms/step - loss: 6.5900 - acc: 0.7804 - val_loss: 7.3332 - val_acc: 0.4508\n",
      "Epoch 19/25\n",
      "102/102 [==============================] - 49s 480ms/step - loss: 6.5704 - acc: 0.7826 - val_loss: 7.3116 - val_acc: 0.4381\n",
      "Epoch 20/25\n",
      "102/102 [==============================] - 50s 488ms/step - loss: 6.5305 - acc: 0.7897 - val_loss: 7.2974 - val_acc: 0.4514\n",
      "Epoch 21/25\n",
      "102/102 [==============================] - 50s 488ms/step - loss: 6.5096 - acc: 0.7933 - val_loss: 7.2633 - val_acc: 0.4549\n",
      "Epoch 22/25\n",
      "102/102 [==============================] - 50s 486ms/step - loss: 6.4519 - acc: 0.8072 - val_loss: 7.2598 - val_acc: 0.4456\n",
      "Epoch 23/25\n",
      "102/102 [==============================] - 49s 481ms/step - loss: 6.4753 - acc: 0.7965 - val_loss: 7.2618 - val_acc: 0.4381\n",
      "Epoch 24/25\n",
      "102/102 [==============================] - 49s 479ms/step - loss: 6.4329 - acc: 0.8011 - val_loss: 7.2300 - val_acc: 0.4514\n",
      "Epoch 25/25\n",
      "102/102 [==============================] - 49s 483ms/step - loss: 6.4038 - acc: 0.8057 - val_loss: 7.2065 - val_acc: 0.4537\n"
     ]
    }
   ],
   "source": [
    "#Preforming the training by using fit \n",
    "# IMPORTANT NOTE: This will take a few minutes!\n",
    "h = NNet.fit(x=BaseX_train, y=BaseY_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0, validation_data = (BaseX_val, BaseY_val), shuffle=True)\n",
    "#NNet.save(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# NNet.load_weights('Weights_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 39ms/step - loss: 7.7583 - acc: 0.3543\n",
      "test loss, test acc: [7.758265972137451, 0.35428571701049805]\n"
     ]
    }
   ],
   "source": [
    "results = NNet.evaluate(X_test,Y_test)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 2:***</span> *Number of filters* \n",
    "\n",
    "Rebuild the function `get_net` to have as an input argument a list of number of filters in each layers, i.e. for the CNN defined above the input should have been `[64, 128, 128, 256, 256]`. Now train the model with the number of filters reduced by half. What were the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_2 (Permute)          (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1 (Conv2D)            (None, 32, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 16, 16, 64)        64        \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 16, 16, 64)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "Conv2D_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 8, 8, 128)         32        \n",
      "_________________________________________________________________\n",
      "Conv2D_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 8, 8, 128)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "FCN_1 (Dense)                (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "FCN_2 (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "FCN_3 (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,392,772\n",
      "Trainable params: 1,392,612\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "102/102 [==============================] - 25s 237ms/step - loss: 5.0660 - acc: 0.3477 - val_loss: 4.6854 - val_acc: 0.2500\n",
      "Epoch 2/25\n",
      "102/102 [==============================] - 24s 231ms/step - loss: 4.5955 - acc: 0.4770 - val_loss: 4.8079 - val_acc: 0.2500\n",
      "Epoch 3/25\n",
      "102/102 [==============================] - 23s 228ms/step - loss: 4.4849 - acc: 0.5057 - val_loss: 4.8478 - val_acc: 0.2338\n",
      "Epoch 4/25\n",
      "102/102 [==============================] - 23s 230ms/step - loss: 4.3832 - acc: 0.5366 - val_loss: 4.8472 - val_acc: 0.2488\n",
      "Epoch 5/25\n",
      "102/102 [==============================] - 23s 226ms/step - loss: 4.3121 - acc: 0.5619 - val_loss: 4.7999 - val_acc: 0.2494\n",
      "Epoch 6/25\n",
      "102/102 [==============================] - 23s 229ms/step - loss: 4.2472 - acc: 0.5891 - val_loss: 4.7758 - val_acc: 0.2535\n",
      "Epoch 7/25\n",
      "102/102 [==============================] - 23s 225ms/step - loss: 4.1964 - acc: 0.5903 - val_loss: 4.7710 - val_acc: 0.2737\n",
      "Epoch 8/25\n",
      "102/102 [==============================] - 23s 225ms/step - loss: 4.1579 - acc: 0.6199 - val_loss: 4.7489 - val_acc: 0.2911\n",
      "Epoch 9/25\n",
      "102/102 [==============================] - 24s 233ms/step - loss: 4.0791 - acc: 0.6396 - val_loss: 4.7192 - val_acc: 0.2998\n",
      "Epoch 10/25\n",
      "102/102 [==============================] - 23s 226ms/step - loss: 4.0570 - acc: 0.6389 - val_loss: 4.7479 - val_acc: 0.2963\n",
      "Epoch 11/25\n",
      "102/102 [==============================] - 23s 228ms/step - loss: 4.0283 - acc: 0.6445 - val_loss: 4.7389 - val_acc: 0.3015\n",
      "Epoch 12/25\n",
      "102/102 [==============================] - 23s 226ms/step - loss: 4.0089 - acc: 0.6570 - val_loss: 4.7611 - val_acc: 0.2940\n",
      "Epoch 13/25\n",
      "102/102 [==============================] - 23s 229ms/step - loss: 3.9895 - acc: 0.6520 - val_loss: 4.7526 - val_acc: 0.2940\n",
      "Epoch 14/25\n",
      "102/102 [==============================] - 24s 234ms/step - loss: 3.9509 - acc: 0.6814 - val_loss: 4.7726 - val_acc: 0.2876\n",
      "Epoch 15/25\n",
      "102/102 [==============================] - 23s 226ms/step - loss: 3.9236 - acc: 0.6851 - val_loss: 4.7543 - val_acc: 0.2917\n",
      "Epoch 16/25\n",
      "102/102 [==============================] - 23s 230ms/step - loss: 3.8979 - acc: 0.6902 - val_loss: 4.7534 - val_acc: 0.2917\n",
      "Epoch 17/25\n",
      "102/102 [==============================] - 23s 226ms/step - loss: 3.9113 - acc: 0.6826 - val_loss: 4.7572 - val_acc: 0.2905\n",
      "Epoch 18/25\n",
      "102/102 [==============================] - 23s 226ms/step - loss: 3.8597 - acc: 0.6958 - val_loss: 4.7619 - val_acc: 0.2922\n",
      "Epoch 19/25\n",
      "102/102 [==============================] - 24s 233ms/step - loss: 3.8542 - acc: 0.6981 - val_loss: 4.7565 - val_acc: 0.2934\n",
      "Epoch 20/25\n",
      "102/102 [==============================] - 23s 228ms/step - loss: 3.8503 - acc: 0.7020 - val_loss: 4.7454 - val_acc: 0.2969\n",
      "Epoch 21/25\n",
      "102/102 [==============================] - 23s 229ms/step - loss: 3.8170 - acc: 0.7127 - val_loss: 4.7639 - val_acc: 0.2946\n",
      "Epoch 22/25\n",
      "102/102 [==============================] - 23s 228ms/step - loss: 3.8001 - acc: 0.7244 - val_loss: 4.7556 - val_acc: 0.2951\n",
      "Epoch 23/25\n",
      "102/102 [==============================] - 23s 229ms/step - loss: 3.7975 - acc: 0.7144 - val_loss: 4.7382 - val_acc: 0.2998\n",
      "Epoch 24/25\n",
      "102/102 [==============================] - 24s 231ms/step - loss: 3.7940 - acc: 0.7192 - val_loss: 4.7445 - val_acc: 0.2980\n",
      "Epoch 25/25\n",
      "102/102 [==============================] - 23s 229ms/step - loss: 3.7567 - acc: 0.7268 - val_loss: 4.7574 - val_acc: 0.2986\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 5.1231 - acc: 0.2457\n",
      "test loss, test acc: [5.123109817504883, 0.24571429193019867]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "def get_net1(input_shape,drop,dropRate,reg,num_filters):\n",
    "    #Defining the network architecture:\n",
    "    model = Sequential()\n",
    "    model.add(Permute((1,2,3),input_shape = input_shape))\n",
    "    model.add(Conv2D(filters=num_filters[0], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_1',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=num_filters[1], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_2',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:    \n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=num_filters[2], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_3',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=num_filters[3], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_4',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=num_filters[4], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_5',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    #Fully connected network tail:      \n",
    "    model.add(Dense(512, activation='elu',name='FCN_1')) \n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(Dense(128, activation='elu',name='FCN_2'))\n",
    "    model.add(Dense(4, activation= 'softmax',name='FCN_3'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "num_filters = [32, 64, 64, 128, 128]\n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 1e-03\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "drop = True\n",
    "dropRate = 0.3\n",
    "reg = 1e-2\n",
    "NNet_new = get_net1(input_shape,drop,dropRate,reg,num_filters)\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "\n",
    "#Compile the network: \n",
    "NNet_new.compile(optimizer=AdamOpt, metrics=['acc'], loss='categorical_crossentropy')\n",
    "\n",
    "\n",
    "h = NNet_new.fit(x=BaseX_train, y=BaseY_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0, validation_data = (BaseX_val, BaseY_val), shuffle=True)\n",
    "results = NNet_new.evaluate(X_test,Y_test)\n",
    "print('test loss, test acc:', results)\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks! See you :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
